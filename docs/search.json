[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mihnea POPA",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download PDF file.Download CV\nDownload PDF file.\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nOn r√©sout l‚Äô√©quation :\n\\[\nx^2 - 4 = 0\n\\]\nDonc :\n\\[\nx = \\pm 2\n\\]\n\n\n\n\n\nVoir la solution\n\nOn r√©sout :\n\\[x^2 - 4 = 0\\]\nDonc \\[x = \\pm 2\\]"
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let‚Äôs investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of -0.85.\n\n\nHere is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Detecting the drowsiness at the wheel\n\n\n\n\n\nMihnea POPA\n\n\nMay 1, 2022\n\n\n\n\n\n\n\n\n\n\n\nModelling a linear time series by an ARIMA model and prediction\n\n\n\n\n\nMihnea POPA / Paul LEMOINE\n\n\nMay 1, 2025\n\n\n\n\n\n\n\n\n\n\n\nPricing options with Black-Sholes-Merton and Monte Carlo in C++\n\n\n\n\n\nMihnea POPA\n\n\nDec 1, 2024\n\n\n\n\n\n\n\n\n\n\n\nThesis on descriptive statistics on taxation in France\n\n\n\n\n\nMihnea POPA / Tristan FABRE\n\n\nDec 1, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/project2/index.html",
    "href": "projects/project2/index.html",
    "title": "Modelling a linear time series  by an ARIMA model and prediction",
    "section": "",
    "text": "The selected series corresponds to the industrial production index (base 2021) in the sector of Woodworking and manufacture of wood and cork products, except furniture, manufacture of basketry and wickerwork products. It is characterized by a monthly frequency and covers the period from January 1990 to February 2025. This is a CVS-CJO index, which means that the series is adjusted for seasonal and working day variations, so it is already pre-processed to eliminate regular seasonal effects. INSEE - Monthly Data\nDownload PDF file.Download Report Download PDF file.\n\n# Script avec commentaires pour le projet de s√©ries temporelles Paul / Mihnea\n# Premi√®re version: 01 Mai 2025 \n\n#==============================================================================#\n# Packages ---------------------------------------------------------------------\n#==============================================================================#\n\nlibrary(zoo)\nlibrary(tseries)\nlibrary(fUnitRoots)\nlibrary(purrr)\nlibrary(lmtest)\n\n#==============================================================================#\n# Partie 1 Les donn√©es ---------------------------------------------------------\n#==============================================================================#\n\n  #Importation et nettoyage de la base\ndata &lt;- read.csv(\"valeurs_mensuelles.csv\", sep = \",\")\ndata &lt;- data[-c(1:3), ] #Enlever les premi√®res lignes qui donnent l'ID de la base et la date de mise √† jour\ndata &lt;- data[,-c(3) ] #Enlever la derni√®re colonne avec les codes de chaque observation\ncolnames(data) &lt;- c(\"dates\",\"value\")\ndates_char &lt;- as.character(data$dates)\ndata$value &lt;- as.numeric(data$value)\ndates_char[[1]]; dates_char[length(dates_char)]\n\n[1] \"1990-01\"\n\n\n[1] \"2025-12\"\n\ndates &lt;- as.yearmon(seq(from=1990, to=2025+11/12, by=1/12))\nvalue &lt;- zoo(data$value, order.by=dates)\n\n  #Visualisation de la s√©rie temporelle et de son ACF\n\nplot(value)\n\n\n\n\n\n\n\nacf(value)\n\n\n\n\n\n\n\n      #La s√©rie ne pr√©sente pas de saisonnalit√© au vu de son √©volution et de son ACF.\n      #Cependant, graphiquement nous remarquons qu'elle n'est pas stationnaire.\n\n  #Diff√©renciation de la s√©rie et visualisation de dvalue\ndvalue &lt;- diff(value,1)\nplot(dvalue)\n\n\n\n\n\n\n\nacf(dvalue)\n\n\n\n\n\n\n\n      # La s√©rie diff√©renci√©e semble stationnaire et ne pr√©sente pas de saisonnalit√©\n      # Pas de signe visible √† l'oeil de tendance, donc a priori, la s√©rie n'est pas\n      # int√©gr√©e, i.e. n'a pas une racine unitaire.\n      # On va toutefois tester √ßa plus pr√©cis√©ment avec un test de racine unitaire \n\n  #Mod√®le de regression lin√©aire pour savoir quel test de racine unitaire faire\ndvalue &lt;- zoo(dvalue, order.by=dates)\nsummary(lm(dvalue ~ dates))\n\n\nCall:\nlm(formula = dvalue ~ dates)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.063  -2.131  -0.124   2.090  31.798 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) 17.592350  46.996939   0.374    0.708\ndates       -0.008821   0.023405  -0.377    0.706\n\nResidual standard error: 5.055 on 430 degrees of freedom\nMultiple R-squared:  0.0003302, Adjusted R-squared:  -0.001995 \nF-statistic: 0.142 on 1 and 430 DF,  p-value: 0.7064\n\n      #Au vu de la r√©gression lin√©aire, nous sommes dans le cas pas centr√© et pas de tendance visible\n      #On va alors effectuer un test de Perron-Phillips pour v√©rifier la pr√©sence ou non de racine unitaire\npptest_case_stationary &lt;- pp.test(dvalue, alternative = \"stationary\")\npptest_case_stationary$p.value\n\n[1] 0.01\n\n      #On obtient une p-value de 0.01 donc au seuil 98.9% on rejette H0 donc on accepte H1\n      #Dans le test de PP l'hypoth√®se H1 est \"il n'y a pas de racine unitaire\".\n      #On en conclu que la s√©rie diff√©renci√©e dvalue est stationnaire. Pour en √™tre s√ªr on peut faire un test de kpss\nkpss.test(dvalue, null = \"Level\")$p.value\n\n[1] 0.1\n\n      #La p-value obtenue est 0.1 donc au seuil 99% (m√™me 95%) on ne rejette pas H0 donc dvalue est bien stationnaire\n\n  #Repr√©sentation graphique de la s√©rie avant et apr√®s transformation\n#windows()\n#par(mfrow = c(2, 1))\nplot(value)\n\n\n\n\n\n\n\nplot(dvalue)\n\n\n\n\n\n\n\n#==============================================================================#\n# Partie 2 Mod√®les ARMA --------------------------------------------------------\n#==============================================================================#\n\n  #On commence par centrer la variable dvalue\ndvalue_centree &lt;- dvalue - mean(dvalue)\n\n  #On regarde l'ACF de cette nouvelle variable pour d√©terminer l'ordre maximal du c√¥t√© MA\nacf(dvalue_centree)\n\n\n\n\n\n\n\nq_test=2\n      #L'odre maximal du c√¥t√© MA est alors q_test = 2\n\n  #On regarde le PACF de cette nouvelle variable pour d√©terminer l'ordre maximal du c√¥t√© AR\npacf(dvalue_centree)\n\n\n\n\n\n\n\np_test=3\n      #L'odre maximal du c√¥t√© AR est alors p_test = 3\n\n  #Cr√©ation de la fonction de test Box-Ljung pour v√©rifier que les r√©sidus ne sont pas corr√©l√©s\nQtests &lt;- function(series, nb_lags_max_Portmanteau_test = 24, fitdf = 0) {\n  pvals &lt;- apply(X = matrix(1:nb_lags_max_Portmanteau_test), MARGIN = 1, \n                 FUN = function(l) {\n                   if (l &lt;= fitdf) {\n                     pval &lt;- NA\n                   } else {\n                     pval &lt;- Box.test(series, lag = l, type = \"Ljung-Box\", fitdf = fitdf)$p.value\n                   }\n                   return(c(\"lag\" = l, \"pval\" = pval))\n                 })\n  return(t(pvals))\n}\n\n  #On cr√©e tous les mod√®les possibles pour p&lt;=p_test et q&lt;=q_test\npqs &lt;- expand.grid(0:p_test,0:q_test)\nmat &lt;- matrix(NA, nrow=p_test +1, ncol=q_test +1)\nrownames(mat) &lt;- paste0(\"p=\",0:p_test) #renomme les lignes\ncolnames(mat) &lt;- paste0(\"q=\",0:q_test) #renomme les colonnes\nAICs &lt;- mat #matrice ou assigner les AIC\nBICs &lt;- mat #matrice ou assigner les BIC\nfor (row in 1:dim(pqs)[1]){\n  p &lt;- pqs[row,1]\n  q &lt;- pqs[row,2]\n  if ((p==0)&&(q==2)) {print(c(p,q))}\n  estim &lt;- arima(dvalue_centree,c(p,0,q), include.mean=F) #tente d'estimer l'ARIMA\n \n  q_tests = Qtests(estim$residuals, nb_lags_max_Portmanteau_test = 30, \n         fitdf = length(estim$coef))\n  if ((p==0)&&(q==2)) {print(q_tests)}\n  if ((p==0)&&(q==2)) {print(coeftest(estim))}\n  AICs[p+1,q+1] &lt;- if (class(estim)==\"try-error\") NA else estim$aic\n  BICs[p+1,q+1] &lt;- if (class(estim)==\"try-error\") NA else BIC(estim)\n}\n\n[1] 0 2\n      lag      pval\n [1,]   1        NA\n [2,]   2        NA\n [3,]   3 0.5524487\n [4,]   4 0.8336335\n [5,]   5 0.7997667\n [6,]   6 0.8927779\n [7,]   7 0.7102150\n [8,]   8 0.8155912\n [9,]   9 0.8853309\n[10,]  10 0.9255650\n[11,]  11 0.9285630\n[12,]  12 0.9342528\n[13,]  13 0.9439155\n[14,]  14 0.9210362\n[15,]  15 0.9066371\n[16,]  16 0.9380520\n[17,]  17 0.9202788\n[18,]  18 0.9461022\n[19,]  19 0.8702773\n[20,]  20 0.9037391\n[21,]  21 0.9272577\n[22,]  22 0.9447795\n[23,]  23 0.9615493\n[24,]  24 0.9036737\n[25,]  25 0.9262018\n[26,]  26 0.9431918\n[27,]  27 0.9473225\n[28,]  28 0.9574999\n[29,]  29 0.9694680\n[30,]  30 0.9739654\n\nz test of coefficients:\n\n     Estimate Std. Error z value  Pr(&gt;|z|)    \nma1 -0.307170   0.046560 -6.5973 4.187e-11 ***\nma2 -0.205355   0.046174 -4.4474 8.692e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n  #On d√©termine la validit√© des mod√®les\n\n      #Avec les tests du porte-manteau, on en d√©duit que les mod√®les o√π les r√©sidus sont bien d√©cor√©ll√©s au seuil 99% sont:\n        # ARMA( 3 , 0 ) ; ARMA( 1 , 1 ) ; ARMA( 2 , 1 ) ; ARMA( 3 , 1 ) ; ARMA( 0 , 2 ) ; ARMA( 1 , 2 ) ; ARMA( 2 , 2 ) ; ARMA( 3 , 2 ) ;\n      #AVec les tests de Student, on en d√©duit que les mod√®les o√π les coefficients de plus grand ordre sont significatifs sont:\n        # ARMA( 1 , 0 ) ; ARMA( 2 , 0 ) ; ARMA( 0 , 1 ) ; ARMA( 1 , 1 ) ; ARMA( 0 , 2 )\n\n      #Ainsi, les mod√®les possibles pour dvalue_centr√©e (ceux qui passent les deux tests de validit√©s) sont:\n        # ARMA( 1 , 1 ) ; ARMA( 0 , 2 )\n  \n  #On d√©termine les meilleurs mod√®les avec AIC , BIC\nAICs\n\n         q=0      q=1      q=2\np=0 2626.188 2594.061 2577.748\np=1 2609.066 2579.489 2579.379\np=2 2589.315 2579.415 2581.357\np=3 2584.691 2581.342 2582.358\n\n        #Au sens de AIC, le meilleur mod√®le est ARMA(0 , 2 )\nBICs\n\n         q=0      q=1      q=2\np=0 2630.256 2602.198 2589.953\np=1 2617.203 2591.694 2595.653\np=2 2601.520 2595.689 2601.699\np=3 2600.965 2601.684 2606.769\n\n        #Au sens de BIC, le meilleur mod√®le est ARMA( 0 , 2) \narima002 &lt;- arima(dvalue_centree, order = c(0, 0, 2), include.mean = FALSE)\narima101 &lt;- arima(dvalue_centree, order = c(1, 0, 1), include.mean = FALSE)\n\n  #On d√©termine le meilleur mod√®le parmi les deux pr√©c√©dants avec le crit√®re du R^2 ajust√©\n\nAdj_R2 &lt;- function(model){\n  p &lt;- model$arma[[1]] # ordre AR\n  q &lt;- model$arma[[2]] # ordre MA\n  ss_res &lt;- sum(model$residuals[-1]^2) # Modele ARMA sur dspread, perd premi√®re date\n  ss_tot &lt;- sum(dvalue_centree^2)\n  n &lt;- length(dvalue_centree)\n  adj_r2 &lt;- 1 - (ss_res / (n - (p + q + 1))) / (ss_tot / (n - 1))\n  return(adj_r2)\n}\nAdj_R2(arima101)\n\n[1] 0.110115\n\nAdj_R2(arima002)\n\n[1] 0.1136935\n\n      #Au sens du crit√®re du R^2, le meilleur mod√®le est ARMA( 0 , 2 )\n\n#### Par cons√©quent, le meilleur mod√®le pour value est ARIMA(0,1,2)\nvalue &lt;- value\nmodel &lt;- arima(value,c(0,1,2), include.mean=F)\n\n#==============================================================================#\n# Partie 3 Pr√©diction ----------------------------------------------------------\n#==============================================================================#\n\n#Avec le mod√®le ARIMA(0,1,2), la pr√©diction devenait constante √† partir de l'horizon 2\n#Ainsi, on a chois de prendre l'autre mod√®le valide √† savoir le mod√®le ARIMA(1,1,1)\n\n#Avoir le theta, le phi et le sigma avec le mod√®le ARIMA(1,1,1)\nsigma2 &lt;- arima101$sigma2\nphi &lt;- coef(arima101)[\"ar1\"]  \ntheta &lt;- coef(arima101)[\"ma1\"]  \n\n#Fixer les param√®tres de la pr√©diction\nn_value &lt;- length(value)\nhorizon &lt;- 5\nstart_pred &lt;- n_value - horizon\nvalue_vraie &lt;- as.numeric(value)\n\n#Effectuer la pr√©diction\nfit &lt;- arima(value_vraie[1:(n_value - horizon)], order = c(1, 1, 1))\n \nprediction &lt;- function(phi, theta, value_vraie, start_pred, horizon) {\n  val1 &lt;- value_vraie[start_pred]   #t_X_t                          \n  val2 &lt;- val1 + phi * (val1 - value_vraie[start_pred - 1]) \n  var_vec &lt;- numeric(horizon + 1)\n  #t_X_t+1\n  var_vec[1]  &lt;- val1 \n  var_vec[2]  &lt;- val2 \n  # Calculer les valeurs pour h = 2 √† horizon\n  for (h in 2:horizon) {\n    var_vec[h+1] &lt;- (1 / (1 - phi)) * (val2 - phi * val1 + (val1 - val2) * phi^h) #t_X_t+h\n  } \n  # Retourner le vecteur complet\n  return(var_vec)\n}\n#Calculer la variance des √©carts\nvariance &lt;- function(phi, theta, sigma2, horizon) {\n  # Initialise vecteur r√©sultats\n  var_vec &lt;- numeric(horizon + 1)\n  var_vec[1] &lt;- 0  # pour h=0\n  \n  for (h in 1:horizon) {\n    sum_k &lt;- 0\n    for (k in 0:(h-1)) {\n      val_k &lt;- (1 - phi^(k+1) + theta + theta * phi^k) / (1 - phi)\n      sum_k &lt;- sum_k + val_k^2\n    }\n    var_vec[h+1] &lt;- sigma2 * sum_k\n  }\n  \n  return(var_vec)\n}\n\n#Cr√©er la base avec toutes les infos de la pr√©dition\n\npred &lt;- data.frame(\n  pred = prediction(phi, theta, value_vraie, start_pred, horizon),\n  se = variance(phi, theta, sigma2, horizon))\n\n\n# Cr√©er la s√©rie pr√©dite et la s√©rie des bornes de l'intervalle de confiance\npredicted_values &lt;- ts(pred$pred, start = start_pred)\nlower_95 &lt;- ts(pred$pred - 1.96 * pred$se, start = start_pred)\nupper_95 &lt;- ts(pred$pred + 1.96 * pred$se, start = start_pred)\n\n# Tracer la s√©rie originale et les pr√©dictions\nplot(value_vraie, type=\"l\", xlim = c(n_value - 2 * horizon, n_value), ylim = range(c(value_vraie[(n_value - 2*horizon):n_value], lower_95, upper_95)+c(-10,10), na.rm = TRUE), #Enlever le lower_95 et le upper_95 pour une meilleur fen√™tre\n     col = \"black\", lwd = 2, main = \"Pr√©vision ARIMA(1,1,1) pour les derni√®res valeurs\",\n     ylab = \"Valeur\", xlab = \"Temps\")\n\n# Ajouter les pr√©dictions et les intervalles\nlines(predicted_values, col = \"blue\", lwd = 2)\nlines(lower_95, col = \"red\", lty = 2)\nlines(upper_95, col = \"red\", lty = 2)\n\nlegend(\"topleft\", legend = c(\"S√©rie r√©elle\", \"Pr√©diction\", \"Intervalle 95%\"),\n       col = c(\"black\", \"blue\", \"red\"), lty = c(1,1,2), lwd = 2)\n\n\n\n\n\n\n\n# Tracer la s√©rie originale et les pr√©dictions\nplot(value_vraie, type=\"l\", xlim = c(n_value - 2 * horizon, n_value), ylim = range(c(value_vraie[(n_value - 2*horizon):n_value])+c(-10,10), na.rm = TRUE), #Enlever le lower_95 et le upper_95 pour une meilleur fen√™tre\n     col = \"black\", lwd = 2, main = \"Pr√©vision ARIMA(1,1,1) pour les derni√®res valeurs\",\n     ylab = \"Valeur\", xlab = \"Temps\")\n\n# Ajouter les pr√©dictions et les intervalles\nlines(predicted_values, col = \"blue\", lwd = 2)\nlines(lower_95, col = \"red\", lty = 2)\nlines(upper_95, col = \"red\", lty = 2)\n\nlegend(\"topleft\", legend = c(\"S√©rie r√©elle\", \"Pr√©diction\", \"Intervalle 95%\"),\n       col = c(\"black\", \"blue\", \"red\"), lty = c(1,1,2), lwd = 2)"
  },
  {
    "objectID": "projects/project2/index.html#sub-header",
    "href": "projects/project2/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects/fiscalite/index.html",
    "href": "projects/fiscalite/index.html",
    "title": "Thesis on descriptive statistics on taxation in France",
    "section": "",
    "text": "Presentation of the project\nThis thesis corresponds to descriptive statistics established using the ESS survey and SAS language in order to examine which taxation system best suits which society. It will first study the relationship between trust in institutions and support for a given tax policy. It will then show that this support also depends on more concrete variables such as age, income, or education level. Finally, we will compare tax preferences to determine, without going as far as ideals, the strong values that govern societies. In other words, tax norms are always based on social values that need to be updated.\nDownload PDF file.Download Report\n\n\nPresentation of the report\nIn this part, you can display the report and read it without downloading it.\n\n\nShow the report\n\nDownload PDF file.\n\n\n\nPresentation of the code\nThis part will allow you to navigate through the SAS code made in order to have the plots in the thesis. The code is separated in three parts (Import the data, Structure the data, Use the data), each one with multiple subparts.\n\nPart 1: Import the ESS data\n\n\nImport the data with the chosen variables\n\nCr√©ation de la libraire o√π se situe la base de donn√©es\nlibname ess \"W:\\Telechargements\";\nImportation de la base de donn√©es avec les variables retenues\ndata ess;\nset ess.ess;\nkeep \ncntry /*Country*/\ngincdif /*Government should reduce differences in income levels (opinion)*/\nagea /*Age of respondent, calculated*/\nhinctnta /*Household's total net income, all sources*/\nsmdfslv /*For fair society, differences in standard of living should be small*/\nsblazy /*Social benefits/services make people lazy*/\nsblwcoa /*Social benefits/services make people less willing care for one another*/\nsblwlka /*Social benefits/services make people less willing look after themselves/family*/\ntxautef /*Tax authorities, how efficient in doing their job*/\nditxssp /*Government decrease/increase taxes and social spending*/\ntxearn /*Taxation for higher versus lower earners*/\nearnpen /*Higher or lower earners should get larger old age pensions*/\nearnueb /*Higher or lower earners should get larger unemployment benefits*/\nlbenent /*Many with very low incomes get less benefit than legally entitled to*/\ninsfben /*Insufficient benefits in country to help people in real need*/\ntrstprl /*Trust in country's parliament*/\nedulvla /*Highest level of education*/\ntrstlgl /*Trust in the legal system*/\ntrstplt /*Trust in politicians*/\ntrstprt /*Trust in political parties*/\nimprich /*Important to be rich, have money and expensive things*/\nipeqopt /*Important that people are treated equally and have equal opportunities*/\nimpfree /*Important to make own decisions and be free*/\niphlppl /*Important to help people and care for others well-being*/\ngvjbevn /*Job for everyone, governments' responsibility*/\ngvhlthc /*Health care for the sick, governments' responsibility*/\ngvslvol /*Standard of living for the old, governments' responsibility*/\ngvslvue /*Standard of living for the unemployed, governments' responsibility*/\ngvcldcr /*Child care services for working parents, governments' responsibility*/\ngvpdlwk /*Paid leave from work to care for sick family, governments' responsibility*/;\nRUN;\n\n\n\nPart 2: Setup of the variables\n\n\nAdd labels to the variables\n\nRenommage des variables avec des labels\ndata ess_label;\nset ess;\nlabel\ncntry = \"Pays de r√©sidence\"\nagea = \"Age\"\ngincdif = \"Le gouvernement doit prendre des mesures pour r√©duire les in√©galit√©s de revenu (1 = D'accord et 5 = Pas d'accord)\"\nhinctnta = \"Revenu\"\nsmdfslv = \"Pour une soci√©t√© juste, les diff√©rences de niveau de vie doivent √™tre faibles (1 = D'accord et 5 = Pas d'accord)\"\nsblazy = \"Les prestations sociales et les services sociaux rendent les personnes paresseuses (1 = Totalement d'accord et 5 = Totalement pas d'accord)\"\nsblwcoa = \"Les prestations sociales et les services sociaux conduisent les personnes √† moins prendre soin des autres (1 = Totalement d'accord et 5 = Totalement pas d'accord)\"\nsblwlka = \"Les prestations sociales et les services sociaux conduisent les personnes √† moins prendre soin de leurs proches (1 = Totalement d'accord et 5 = Totalement pas d'accord)\"\ntxautef = \"Les autorit√©s fiscales sont efficaces (10 = Tr√®s efficaces)\"\nditxssp = \"Il faut augmenter les imp√¥ts et les prestations sociales \"\ntxearn = \"Impots proportionnels (1), progressifs (2) ou √©gal (3) ?\"\nearnpen = \"Retraite en fonction du salaire (1 = Riche retraite plus √©lev√©e, 2 = Retraite √©gale et 3 = Riche retraite moins √©lev√©e\"\nearnueb = \"Les hauts salaires doivent-ils avoir une aide au ch√¥mage plus √©lev√©e (1), √©gale (2) ou moins √©lev√©e (3) ?\"\nlbenent = \"Beaucoup de personnes √† tr√®s faible revenu per√ßoivent moins d'aides que ce dont ils ont acc√®s\"\ntrstprl = \"Avez-vous confiance dans votre parlement ? (0 = Pas confiance et 10 = Confiance)\"\nedulvla = \"Niveau √©ducation maximal\"\ntrstlgl = \"Confiance dans le syst√®me l√©gal\"\ntrstplt = \"Confiance dans les politiciens\"\ntrstprt = \"Confiance dans les partis politiques\"\nimprich = \"Importance d'√™tre riche (1=Oui et 6=Non)\"\nipeqopt = \"Importance d'√™tre √©gaux (1=Oui et 6=Non)\"\nimpfree = \"Importance d'√™tre libre (1=Oui et 6=Non)\"\niphlppl = \"Importance d'aider les autres (1=Oui et 6=Non)\"\ngvjbevn = \"Responsabilit√© du gouvernement pour les emplois pour tous\"\ngvhlthc = \"Resp du gouv pour le syst√®me de sant√©\"\ngvslvol = \"Resp du gouv pour le niveau de vie des personnes √¢g√©es\"\ngvslvue = \"Resp du gouv pour le niveau de vie des ch√¥meurs\"\ngvcldcr = \"Resp du gouv pour le soin aux enfants de travailleurs\"\ngvpdlwk = \"Resp du gouv jour off pay√© afin de prendre soin famille malade\";\nRUN;\n\n\n\nAdd formats to the variables\n\nCr√©ation de formats\n\nPROC FORMAT;\nvalue income\n1-5 = \"Classe populaire et moyenne inf√©rieure\"\n6-8 = \"Classe moyenne\"\n9-10 = \"Classe sup√©rieure\"\n77 = \"A refus√© de r√©pondre\"\n88 = \"Ne sais pas\"\n99 = \"Pas de r√©ponse\";\nrun;\n\nPROC FORMAT;\nvalue age\n0-&lt;15 = \"Enfant ou adolescent\"\n15-&lt;30 = \"Moins de 30 ans\"\n30-65 = \"Entre 30 et 65 ans\"\n65-500 = \"Plus de 65 ans\"\n999 = \"Pas de r√©ponse\";\nRUN;\n\nPROC FORMAT;\nvalue $country\n\"DK\",\"SE\",\"NO\",\"FI\" = \"Mod√®le Social-D√©mocrate\"\n\"FR\",\"BE\",\"LU\",\"DE\",\"AT\" = \"Mod√®le Conservateur/Corporatiste\"\n\"GB\",\"CY\",\"IE\"=\"Mod√®le Lib√©ral\"\n\"AL\",\"HR\",\"ME\",\"MK\",\"SI\",\"XK\",\"SK\",\"CH\",\"NL\",\"GE\",\"IL\",\"TR\",\"EE\",\"IS\",\"RO\",\"RS\",\"BG\",\n\"CZ\",\"HU\",\"PL\",\"RU\",\"UA\",\"LV\",\"LT\",\"ES\",\"IT\",\"GR\",\"PT\" = \"Autre\";\nRUN;\n\nPROC FORMAT;\nvalue presta_sociale\n1 = \" Absolument d'accord\"\n2 = \"Plut√¥t d'accord\"\n3 = \"Indiff√©rent\"\n4 = \"Plut√¥t pas d'accord\"\n5 = \"Absolument pas d'accord\"\n7 = \"A refus√© de r√©pondre\"\n8 = \"Ne sais pas\"\n9 = \"Pas de r√©ponse\";\nrun;\n\nPROC FORMAT;\nvalue resp_gouv\n0-2 = \"Absolument pas d'accord\"\n3-4 = \"Plut√¥t pas d'accord\"\n5 = \"Sans opinion\"\n6-7 = \"Plut√¥t d'accord\"\n8-10 = \"Absolument d'accord\"\n77 = \"A refus√© de r√©pondre\"\n88 = \"Ne sais pas\"\n99 = \"Pas de r√©ponse\";\nRUN;\n\nPROC FORMAT;\nvalue gouv_evolution_impot_ps\n0-2 = \"Absolument pas d'accord\"\n3-4 = \"Plut√¥t pas d'accord\"\n5 = \"Sans opinion\"\n6-7 = \"Plut√¥t d'accord\"\n8-10 = \"Absolument d'accord\"\n77 = \"A refus√© de r√©pondre\"\n88 = \"Ne sais pas\"\n99 = \"Pas de r√©ponse\";\nRUN;\n\nPROC FORMAT;\nvalue aides\n1 = \"Absolument d'accord\"\n2 = \"Plut√¥t d'accord\"\n3 = \"Indiff√©rent\"\n4 = \"Plut√¥t pas d'accord\"\n5 = \"Absolument pas d'accord\"\n7 = \"A refus√© de r√©pondre\"\n8 = \"Ne sais pas\"\n9 = \"Pas de r√©ponse\";\nRUN;\n\nPROC FORMAT;\nvalue comment_taxer\n1 = \"Imp√¥t proportionnel\"\n2 = \"Imp√¥t progressif\"\n3 = \"Imp√¥t √©gal\"\n4 = \"Aucun de ces r√©ponse\"\n7 = \"A refus√© de r√©pondre\"\n8 = \"Ne sais pas\"\n9 = \"Pas de r√©ponse\";\nRUN;\n\nPROC FORMAT;\nvalue education_ISCED\n0 = \"Impossible d'harmoniser\"\n1 = \"Moins que secondaire\"\n2-3 = \"Secondaire\"\n4-5 = \"Sup√©rieur\"\n55 = \"Autre\"\n77 = \"A refus√© de r√©pondre\"\n88 = \"Ne sais pas\"\n99 = \"Pas de r√©ponse\";\nRUN;\n\nPROC FORMAT;\nvalue gouv\n0-3 = \"Pas d'accord\"\n4-6 = \"Sans opinion\"\n7-10 = \"D'accord\"\n77 = \"A refus√© de r√©pondre\"\n88 = \"Ne sais pas\"\n99 = \"Pas de r√©ponse\";\nRUN;\n\nPROC FORMAT;\nvalue imp\n0-2 = \"Important\"\n2-4 = \"Neutre\"\n4-6 = \"Pas important\"\n7 = \"A refus√© de r√©pondre\"\n8 = \"Ne sais pas\"\n9 = \"Pas de r√©ponse\";\nRUN;\n\n\n\nPart 3: Make the plots\n\n\nCreate function for the plots\n\nCr√©ation des macros que l‚Äôon va utiliser\n\n%MACRO panel_var1_var2(variable1,variable2,format1,format2,maxvar1,maxvar2);/*Cette macro cr√©e un panneau suivant les valeurs d'une variable \"variable1\" cod√©e par le format \"format1\" et dans chaque case du panneau repr√©sente les proportions des valeurs de la variable \"variable2\" cod√©e par le format \"format2\"*/\n\n  /*Tri de la table suivant la variable \"variable1\"*/\n  proc sort data=ess_label;\n    by &variable1;\n  run;\n  /*Calcul des pourcentages de chaque valeur de la variable \"variable2\" en groupant selon les valeurs de la variable \"variable1\"*/\n  proc freq data=ess_label\n  (WHERE = (cntry=\"FR\" and &variable1&lt;=&maxvar1 and &variable2&lt;=&maxvar2))\n  noprint;\n    format &variable1 %scan(&format1, 1, %str(%bquote(%')%str(%\"))).;\n    by &variable1;\n    tables &variable2 / out=FreqOut;\n  run;\n  \n  /*Cr√©ation du panneau suivant les valeurs de la variable \"variable1\" en repr√©sentant les proportions des valeurs de la variable \"variable2\" */\n  proc sgpanel data=FreqOut;\n    format &variable1 %scan(&format1, 1, %str(%bquote(%')%str(%\"))).;\n    format &variable2 %scan(&format2, 1, %str(%bquote(%')%str(%\"))).;\n    styleattrs datacontrastcolors=(red green blue) datacolors=(red green blue);\n    panelby &variable1 / columns = 3 novarname noheader;\n    vbar &variable2 /group=&variable1 barwidth=0.5\n  response=percent datalabel ;\n  run;\n  \n%MEND;\n\n\n%MACRO barres_stack(variable);/*Cette macro permet pour une variable \"variable\", qui sera une variable de responsabilit√© du gouvernement, de faire un histogramme horizontal avec barres empil√©es pour repr√©senter cette variable pour les diff√©rentes classes de revenu*/\n\n  /*Tri de la table suivant le revenu avec les observations utiles*/\n  proc sort data=ess_label (WHERE=(cntry= \"FR\" and hinctnta&lt;=10 and &variable&lt;=10 )) out=ess_label_bis ;\n    format hinctnta income.;\n    by hinctnta;\n  run;\n  \n  /*Calcul des pourcentages de chaque valeur de la variable \"variable\" en groupant selon les classes de revenu*/\n\n  proc freq data=ess_label_bis noprint;\n    by hinctnta;\n    tables &variable / out=FreqOut;\n  run;\n  \n  /*Cr√©ation de l'histogramme horizontal avec barres empil√©es*/\n  proc sgplot data=FreqOut NOAUTOLEGEND;\n    styleattrs datacontrastcolors=(red green blue) datacolors=(red green blue);\n    format hinctnta income.;\n    format &variable gouv.;\n    hbar hinctnta / response=Percent group=&variable groupdisplay=stack barwidth=0.5 ;\n    xaxis grid values=(0 to 100 by 10);\n  run;\n  \n%MEND;\n\n\n%MACRO trust(variable1,variable2,maxvar2); /*Cette macro √©tudie le lien entre diff√©rentes variables de confiances \"variable1\" en abscisse et \"variable2\" en ordonn√©es. Elle fait une moyenne sur les pays et repr√©sente une droite de tendance*/\n\n  /*Calcule la moyenne de la variable \"variable1\" pour chaque pays*/\n  proc means data=ess_label (where = (&variable2 &lt;=&maxvar2 and &variable1&lt;50)) noprint;\n    class cntry;\n    var &variable1 &variable2 ;\n    output out=mean_data_2 mean=;\n  run;\n  /*Repr√©sente en abscisses la variable \"variable1\" et en ordonn√©e \"variable2\" avec la droite de tendance*/\n  proc sgplot data=mean_data_2;\n    scatter x=&variable1 y=&variable2 / group=cntry;\n    keylegend / location=outside;\n    reg x=&variable1 y=&variable2 / lineattrs=(color=red) NOMARKERS;\n  run;\n%MEND;\n\n\n%MACRO consensus(variable, format); /*Cette macro √©tudie les fr√©quences de la variable \"variable\" cod√©e par le format \"format\" pour voir s'il existe un consensus aupr√®s des individus*/\n\n  /*Calcul des fr√©quences des valeurs de la variable \"variable\"*/\n  proc freq data=ess_label (WHERE=(cntry=\"FR\" /*and 8&lt;=hinctnta&lt;=10 Pour le dernier consensus */ ));\n    tables %scan(&variable, 1, %str(%bquote(%')%str(%\"))) / out=FreqOut;\n  run;\n  \n/*Cr√©ation del'histogramme pour voir s'il existe un consensus*/\n  proc sgplot data=FreqOut NOAUTOLEGEND;\n    styleattrs datacontrastcolors=(red orange green blue purple red) datacolors=(red orange green blue purple red);\n    VBAR %scan(&variable, 1, %str(%bquote(%')%str(%\"))) /response=percent group=%scan(&variable, 1, %str(%bquote(%')%str(%\")))\n    datalabel; format %scan(&variable, 1, %str(%bquote(%')%str(%\")))\n%scan(&format, 1, %str(%bquote(%')%str(%\"))).;\n  run;\n  \n%MEND;\n\n%MACRO vari_en_fct_pays(variable);/*Cette macro cr√©e un graphique de points avec en abscisse la cat√©gorie de pays (Europe Centrale, Europe du Nord, Lib√©ral, Familialiste) et en ordonn√©e la moyenne de la variable \"variable\" pour chaque pays de chaque cat√©gorie*/\n\n  /*Renvoit la moyenne de la variable \"variable\" pour chaque pays pr√©sent dans la table */\n  proc means data=ess_label (where = (&variable&lt;=4)) noprint;\n    class cntry;\n    var &variable;\n    output out=mean_data mean=;\n  run;\n  /*Selectionne les moyennes qui correspondent aux pays des diff√©rentes cat√©gories √©tudi√©es*/\n  data mean_data;\n    set mean_data;\n    if cntry in (\"DK\",\"SE\",\"NO\",\"FI\",\"FR\",\"BE\",\"LU\", \"DE\",\"AT\",\"GB\",\"CY\",\"IE\");\n  run;\n/*Cr√©ation du graphique de points en regroupant par la cat√©gorie de pays afin d'obtenir des couleurs diff√©rentes*/\n  proc sgplot data=mean_data NOAUTOLEGEND;\n    styleattrs datacontrastcolors=(red green blue) datacolors=(red green blue);\n    format cntry $country.;\n    scatter x=cntry y=&variable /group=cntry;\n  run;\n  \n%MEND;\n\n\n\nUsed plots\n\nGraphiques utilis√©s\n\n/*I.A.1*/\n    %panel_var1_var2(hinctnta,ditxssp,\"income\",\"gouv_evolution_impot_ps\",10,10);\n    %panel_var1_var2(hinctnta,sblazy,\"income\",\"presta_sociale\",10,5);\n    %panel_var1_var2(hinctnta,lbenent,\"income\",\"aides\",10,5);\n    %panel_var1_var2(hinctnta,insfben,\"income\",\"aides\",10,5);\n    \n/*I.A.2*/\n    %barres_stack(gvjbevn);\n    %barres_stack(gvhlthc);\n    %barres_stack(gvslvol);\n    %barres_stack(gvslvue);\n    %barres_stack(gvcldcr);\n    %barres_stack(gvpdlwk);\n    \n/*I.B*/\n    /*Acceptation*/\n        %trust(trstprl,ditxssp,10);\n        %trust(trstlgl,ditxssp,10);\n        %trust(trstplt,ditxssp,10);\n        %trust(trstprt,ditxssp,10);\n    /*M√©thode*/\n        %trust(trstprl,txearn,3);\n        %trust(trstlgl,txearn,3);\n        %trust(trstplt,txearn,3);\n        %trust(trstprt,txearn,3);\n    /*Efficacit√©*/\n        %trust(trstprl,txautef,10);\n        %trust(trstlgl,txautef,10);\n        %trust(trstplt,txautef,10);\n        %trust(trstprt,txautef,10);\n\n/*I.C*/\n    %consensus(\"gincdif\",\"presta_sociale\");\n    %consensus(\"sblwcoa\",\"presta_sociale\");\n    %consensus(\"txearn\",\"comment_taxer\");\n    %consensus(\"ipeqopt\",\"imp\");\n    %consensus(\"smdfslv\",\"presta_sociale\");\n    \n/*II.A*/\n    %panel_var1_var2(hinctnta,ditxssp,\"income\", \"gouv_evolution_impot_ps\",10,10);\n    \n/*II.B*/\n  %panel_var1_var2(edulvla,ditxssp,\"education_ISCED\", \"gouv_evolution_impot_ps\",10,10);\n\n/*II.C*/\n    %panel_var1_var2(agea,ditxssp,\"age\", \"gouv_evolution_impot_ps\",200,10);\n    \n/*III*/\n    %vari_en_fct_pays(imprich);\n    %vari_en_fct_pays(ipeqopt);\n    %vari_en_fct_pays(impfree);\n    %vari_en_fct_pays(iphlppl);\n    %vari_en_fct_pays(earnpen);\n    %vari_en_fct_pays(earnueb);\n    %vari_en_fct_pays(sblwlka);\n    %vari_en_fct_pays(sblazy);"
  },
  {
    "objectID": "projects/serie_temp/index.html",
    "href": "projects/serie_temp/index.html",
    "title": "Modelling a linear time series by an ARIMA model and prediction",
    "section": "",
    "text": "The selected series corresponds to the industrial production index (base 2021) in the sector of Woodworking and manufacture of wood and cork products, except furniture, manufacture of basketry and wickerwork products. It is characterized by a monthly frequency and covers the period from January 1990 to February 2025. This is a CVS-CJO index, which means that the series is adjusted for seasonal and working day variations, so it is already pre-processed to eliminate regular seasonal effects. INSEE - Monthly Data\nDownload PDF file.Download Report\n\n\nShow the report\n\nDownload PDF file.\n\n\n\nShow the code\n\n\n# Script avec commentaires pour le projet de s√©ries temporelles Paul / Mihnea\n# Premi√®re version: 01 Mai 2025 \n\n#==============================================================================#\n# Packages ---------------------------------------------------------------------\n#==============================================================================#\n\nlibrary(zoo)\nlibrary(tseries)\nlibrary(fUnitRoots)\nlibrary(purrr)\nlibrary(lmtest)\n\n#==============================================================================#\n# Partie 1 Les donn√©es ---------------------------------------------------------\n#==============================================================================#\n\n  #Importation et nettoyage de la base\ndata &lt;- read.csv(\"valeurs_mensuelles.csv\", sep = \",\")\ndata &lt;- data[-c(1:3), ] #Enlever les premi√®res lignes qui donnent l'ID de la base et la date de mise √† jour\ndata &lt;- data[,-c(3) ] #Enlever la derni√®re colonne avec les codes de chaque observation\ncolnames(data) &lt;- c(\"dates\",\"value\")\ndates_char &lt;- as.character(data$dates)\ndata$value &lt;- as.numeric(data$value)\ndates_char[[1]]; dates_char[length(dates_char)]\n\n[1] \"1990-01\"\n\n\n[1] \"2025-12\"\n\ndates &lt;- as.yearmon(seq(from=1990, to=2025+11/12, by=1/12))\nvalue &lt;- zoo(data$value, order.by=dates)\n\n  #Visualisation de la s√©rie temporelle et de son ACF\n\nplot(value)\n\n\n\n\n\n\n\nacf(value)\n\n\n\n\n\n\n\n      #La s√©rie ne pr√©sente pas de saisonnalit√© au vu de son √©volution et de son ACF.\n      #Cependant, graphiquement nous remarquons qu'elle n'est pas stationnaire.\n\n  #Diff√©renciation de la s√©rie et visualisation de dvalue\ndvalue &lt;- diff(value,1)\nplot(dvalue)\n\n\n\n\n\n\n\nacf(dvalue)\n\n\n\n\n\n\n\n      # La s√©rie diff√©renci√©e semble stationnaire et ne pr√©sente pas de saisonnalit√©\n      # Pas de signe visible √† l'oeil de tendance, donc a priori, la s√©rie n'est pas\n      # int√©gr√©e, i.e. n'a pas une racine unitaire.\n      # On va toutefois tester √ßa plus pr√©cis√©ment avec un test de racine unitaire \n\n  #Mod√®le de regression lin√©aire pour savoir quel test de racine unitaire faire\ndvalue &lt;- zoo(dvalue, order.by=dates)\nsummary(lm(dvalue ~ dates))\n\n\nCall:\nlm(formula = dvalue ~ dates)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.063  -2.131  -0.124   2.090  31.798 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) 17.592350  46.996939   0.374    0.708\ndates       -0.008821   0.023405  -0.377    0.706\n\nResidual standard error: 5.055 on 430 degrees of freedom\nMultiple R-squared:  0.0003302, Adjusted R-squared:  -0.001995 \nF-statistic: 0.142 on 1 and 430 DF,  p-value: 0.7064\n\n      #Au vu de la r√©gression lin√©aire, nous sommes dans le cas pas centr√© et pas de tendance visible\n      #On va alors effectuer un test de Perron-Phillips pour v√©rifier la pr√©sence ou non de racine unitaire\npptest_case_stationary &lt;- pp.test(dvalue, alternative = \"stationary\")\npptest_case_stationary$p.value\n\n[1] 0.01\n\n      #On obtient une p-value de 0.01 donc au seuil 98.9% on rejette H0 donc on accepte H1\n      #Dans le test de PP l'hypoth√®se H1 est \"il n'y a pas de racine unitaire\".\n      #On en conclu que la s√©rie diff√©renci√©e dvalue est stationnaire. Pour en √™tre s√ªr on peut faire un test de kpss\nkpss.test(dvalue, null = \"Level\")$p.value\n\n[1] 0.1\n\n      #La p-value obtenue est 0.1 donc au seuil 99% (m√™me 95%) on ne rejette pas H0 donc dvalue est bien stationnaire\n\n  #Repr√©sentation graphique de la s√©rie avant et apr√®s transformation\n#windows()\n#par(mfrow = c(2, 1))\nplot(value)\n\n\n\n\n\n\n\nplot(dvalue)\n\n\n\n\n\n\n\n#==============================================================================#\n# Partie 2 Mod√®les ARMA --------------------------------------------------------\n#==============================================================================#\n\n  #On commence par centrer la variable dvalue\ndvalue_centree &lt;- dvalue - mean(dvalue)\n\n  #On regarde l'ACF de cette nouvelle variable pour d√©terminer l'ordre maximal du c√¥t√© MA\nacf(dvalue_centree)\n\n\n\n\n\n\n\nq_test=2\n      #L'odre maximal du c√¥t√© MA est alors q_test = 2\n\n  #On regarde le PACF de cette nouvelle variable pour d√©terminer l'ordre maximal du c√¥t√© AR\npacf(dvalue_centree)\n\n\n\n\n\n\n\np_test=3\n      #L'odre maximal du c√¥t√© AR est alors p_test = 3\n\n  #Cr√©ation de la fonction de test Box-Ljung pour v√©rifier que les r√©sidus ne sont pas corr√©l√©s\nQtests &lt;- function(series, nb_lags_max_Portmanteau_test = 24, fitdf = 0) {\n  pvals &lt;- apply(X = matrix(1:nb_lags_max_Portmanteau_test), MARGIN = 1, \n                 FUN = function(l) {\n                   if (l &lt;= fitdf) {\n                     pval &lt;- NA\n                   } else {\n                     pval &lt;- Box.test(series, lag = l, type = \"Ljung-Box\", fitdf = fitdf)$p.value\n                   }\n                   return(c(\"lag\" = l, \"pval\" = pval))\n                 })\n  return(t(pvals))\n}\n\n  #On cr√©e tous les mod√®les possibles pour p&lt;=p_test et q&lt;=q_test\npqs &lt;- expand.grid(0:p_test,0:q_test)\nmat &lt;- matrix(NA, nrow=p_test +1, ncol=q_test +1)\nrownames(mat) &lt;- paste0(\"p=\",0:p_test) #renomme les lignes\ncolnames(mat) &lt;- paste0(\"q=\",0:q_test) #renomme les colonnes\nAICs &lt;- mat #matrice ou assigner les AIC\nBICs &lt;- mat #matrice ou assigner les BIC\nfor (row in 1:dim(pqs)[1]){\n  p &lt;- pqs[row,1]\n  q &lt;- pqs[row,2]\n  if ((p==0)&&(q==2)) {print(c(p,q))}\n  estim &lt;- arima(dvalue_centree,c(p,0,q), include.mean=F) #tente d'estimer l'ARIMA\n \n  q_tests = Qtests(estim$residuals, nb_lags_max_Portmanteau_test = 30, \n         fitdf = length(estim$coef))\n  if ((p==0)&&(q==2)) {print(q_tests)}\n  if ((p==0)&&(q==2)) {print(coeftest(estim))}\n  AICs[p+1,q+1] &lt;- if (class(estim)==\"try-error\") NA else estim$aic\n  BICs[p+1,q+1] &lt;- if (class(estim)==\"try-error\") NA else BIC(estim)\n}\n\n[1] 0 2\n      lag      pval\n [1,]   1        NA\n [2,]   2        NA\n [3,]   3 0.5524487\n [4,]   4 0.8336335\n [5,]   5 0.7997667\n [6,]   6 0.8927779\n [7,]   7 0.7102150\n [8,]   8 0.8155912\n [9,]   9 0.8853309\n[10,]  10 0.9255650\n[11,]  11 0.9285630\n[12,]  12 0.9342528\n[13,]  13 0.9439155\n[14,]  14 0.9210362\n[15,]  15 0.9066371\n[16,]  16 0.9380520\n[17,]  17 0.9202788\n[18,]  18 0.9461022\n[19,]  19 0.8702773\n[20,]  20 0.9037391\n[21,]  21 0.9272577\n[22,]  22 0.9447795\n[23,]  23 0.9615493\n[24,]  24 0.9036737\n[25,]  25 0.9262018\n[26,]  26 0.9431918\n[27,]  27 0.9473225\n[28,]  28 0.9574999\n[29,]  29 0.9694680\n[30,]  30 0.9739654\n\nz test of coefficients:\n\n     Estimate Std. Error z value  Pr(&gt;|z|)    \nma1 -0.307170   0.046560 -6.5973 4.187e-11 ***\nma2 -0.205355   0.046174 -4.4474 8.692e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n  #On d√©termine la validit√© des mod√®les\n\n      #Avec les tests du porte-manteau, on en d√©duit que les mod√®les o√π les r√©sidus sont bien d√©cor√©ll√©s au seuil 99% sont:\n        # ARMA( 3 , 0 ) ; ARMA( 1 , 1 ) ; ARMA( 2 , 1 ) ; ARMA( 3 , 1 ) ; ARMA( 0 , 2 ) ; ARMA( 1 , 2 ) ; ARMA( 2 , 2 ) ; ARMA( 3 , 2 ) ;\n      #AVec les tests de Student, on en d√©duit que les mod√®les o√π les coefficients de plus grand ordre sont significatifs sont:\n        # ARMA( 1 , 0 ) ; ARMA( 2 , 0 ) ; ARMA( 0 , 1 ) ; ARMA( 1 , 1 ) ; ARMA( 0 , 2 )\n\n      #Ainsi, les mod√®les possibles pour dvalue_centr√©e (ceux qui passent les deux tests de validit√©s) sont:\n        # ARMA( 1 , 1 ) ; ARMA( 0 , 2 )\n  \n  #On d√©termine les meilleurs mod√®les avec AIC , BIC\nAICs\n\n         q=0      q=1      q=2\np=0 2626.188 2594.061 2577.748\np=1 2609.066 2579.489 2579.379\np=2 2589.315 2579.415 2581.357\np=3 2584.691 2581.342 2582.358\n\n        #Au sens de AIC, le meilleur mod√®le est ARMA(0 , 2 )\nBICs\n\n         q=0      q=1      q=2\np=0 2630.256 2602.198 2589.953\np=1 2617.203 2591.694 2595.653\np=2 2601.520 2595.689 2601.699\np=3 2600.965 2601.684 2606.769\n\n        #Au sens de BIC, le meilleur mod√®le est ARMA( 0 , 2) \narima002 &lt;- arima(dvalue_centree, order = c(0, 0, 2), include.mean = FALSE)\narima101 &lt;- arima(dvalue_centree, order = c(1, 0, 1), include.mean = FALSE)\n\n  #On d√©termine le meilleur mod√®le parmi les deux pr√©c√©dants avec le crit√®re du R^2 ajust√©\n\nAdj_R2 &lt;- function(model){\n  p &lt;- model$arma[[1]] # ordre AR\n  q &lt;- model$arma[[2]] # ordre MA\n  ss_res &lt;- sum(model$residuals[-1]^2) # Modele ARMA sur dspread, perd premi√®re date\n  ss_tot &lt;- sum(dvalue_centree^2)\n  n &lt;- length(dvalue_centree)\n  adj_r2 &lt;- 1 - (ss_res / (n - (p + q + 1))) / (ss_tot / (n - 1))\n  return(adj_r2)\n}\nAdj_R2(arima101)\n\n[1] 0.110115\n\nAdj_R2(arima002)\n\n[1] 0.1136935\n\n      #Au sens du crit√®re du R^2, le meilleur mod√®le est ARMA( 0 , 2 )\n\n#### Par cons√©quent, le meilleur mod√®le pour value est ARIMA(0,1,2)\nvalue &lt;- value\nmodel &lt;- arima(value,c(0,1,2), include.mean=F)\n\n#==============================================================================#\n# Partie 3 Pr√©diction ----------------------------------------------------------\n#==============================================================================#\n\n#Avec le mod√®le ARIMA(0,1,2), la pr√©diction devenait constante √† partir de l'horizon 2\n#Ainsi, on a chois de prendre l'autre mod√®le valide √† savoir le mod√®le ARIMA(1,1,1)\n\n#Avoir le theta, le phi et le sigma avec le mod√®le ARIMA(1,1,1)\nsigma2 &lt;- arima101$sigma2\nphi &lt;- coef(arima101)[\"ar1\"]  \ntheta &lt;- coef(arima101)[\"ma1\"]  \n\n#Fixer les param√®tres de la pr√©diction\nn_value &lt;- length(value)\nhorizon &lt;- 5\nstart_pred &lt;- n_value - horizon\nvalue_vraie &lt;- as.numeric(value)\n\n#Effectuer la pr√©diction\nfit &lt;- arima(value_vraie[1:(n_value - horizon)], order = c(1, 1, 1))\n \nprediction &lt;- function(phi, theta, value_vraie, start_pred, horizon) {\n  val1 &lt;- value_vraie[start_pred]   #t_X_t                          \n  val2 &lt;- val1 + phi * (val1 - value_vraie[start_pred - 1]) \n  var_vec &lt;- numeric(horizon + 1)\n  #t_X_t+1\n  var_vec[1]  &lt;- val1 \n  var_vec[2]  &lt;- val2 \n  # Calculer les valeurs pour h = 2 √† horizon\n  for (h in 2:horizon) {\n    var_vec[h+1] &lt;- (1 / (1 - phi)) * (val2 - phi * val1 + (val1 - val2) * phi^h) #t_X_t+h\n  } \n  # Retourner le vecteur complet\n  return(var_vec)\n}\n#Calculer la variance des √©carts\nvariance &lt;- function(phi, theta, sigma2, horizon) {\n  # Initialise vecteur r√©sultats\n  var_vec &lt;- numeric(horizon + 1)\n  var_vec[1] &lt;- 0  # pour h=0\n  \n  for (h in 1:horizon) {\n    sum_k &lt;- 0\n    for (k in 0:(h-1)) {\n      val_k &lt;- (1 - phi^(k+1) + theta + theta * phi^k) / (1 - phi)\n      sum_k &lt;- sum_k + val_k^2\n    }\n    var_vec[h+1] &lt;- sigma2 * sum_k\n  }\n  \n  return(var_vec)\n}\n\n#Cr√©er la base avec toutes les infos de la pr√©dition\n\npred &lt;- data.frame(\n  pred = prediction(phi, theta, value_vraie, start_pred, horizon),\n  se = variance(phi, theta, sigma2, horizon))\n\n\n# Cr√©er la s√©rie pr√©dite et la s√©rie des bornes de l'intervalle de confiance\npredicted_values &lt;- ts(pred$pred, start = start_pred)\nlower_95 &lt;- ts(pred$pred - 1.96 * pred$se, start = start_pred)\nupper_95 &lt;- ts(pred$pred + 1.96 * pred$se, start = start_pred)\n\n# Tracer la s√©rie originale et les pr√©dictions\nplot(value_vraie, type=\"l\", xlim = c(n_value - 2 * horizon, n_value), ylim = range(c(value_vraie[(n_value - 2*horizon):n_value], lower_95, upper_95)+c(-10,10), na.rm = TRUE), #Enlever le lower_95 et le upper_95 pour une meilleur fen√™tre\n     col = \"black\", lwd = 2, main = \"Pr√©vision ARIMA(1,1,1) pour les derni√®res valeurs\",\n     ylab = \"Valeur\", xlab = \"Temps\")\n\n# Ajouter les pr√©dictions et les intervalles\nlines(predicted_values, col = \"blue\", lwd = 2)\nlines(lower_95, col = \"red\", lty = 2)\nlines(upper_95, col = \"red\", lty = 2)\n\nlegend(\"topleft\", legend = c(\"S√©rie r√©elle\", \"Pr√©diction\", \"Intervalle 95%\"),\n       col = c(\"black\", \"blue\", \"red\"), lty = c(1,1,2), lwd = 2)\n\n\n\n\n\n\n\n# Tracer la s√©rie originale et les pr√©dictions\nplot(value_vraie, type=\"l\", xlim = c(n_value - 2 * horizon, n_value), ylim = range(c(value_vraie[(n_value - 2*horizon):n_value])+c(-10,10), na.rm = TRUE), #Enlever le lower_95 et le upper_95 pour une meilleur fen√™tre\n     col = \"black\", lwd = 2, main = \"Pr√©vision ARIMA(1,1,1) pour les derni√®res valeurs\",\n     ylab = \"Valeur\", xlab = \"Temps\")\n\n# Ajouter les pr√©dictions et les intervalles\nlines(predicted_values, col = \"blue\", lwd = 2)\nlines(lower_95, col = \"red\", lty = 2)\nlines(upper_95, col = \"red\", lty = 2)\n\nlegend(\"topleft\", legend = c(\"S√©rie r√©elle\", \"Pr√©diction\", \"Intervalle 95%\"),\n       col = c(\"black\", \"blue\", \"red\"), lty = c(1,1,2), lwd = 2)"
  },
  {
    "objectID": "projects/fiscalite/index.html#part-1-import-the-ess-data",
    "href": "projects/fiscalite/index.html#part-1-import-the-ess-data",
    "title": "Thesis on descriptive statistics on taxation in France",
    "section": "Part 1: Import the ESS data",
    "text": "Part 1: Import the ESS data\n\n\nImport the data with the chosen variables\n\nCr√©ation de la libraire o√π se situe la base de donn√©es\nlibname ess \"W:\\Telechargements\";\nImportation de la base de donn√©es avec les variables retenues\ndata ess;\nset ess.ess;\nkeep \ncntry /*Country*/\ngincdif /*Government should reduce differences in income levels (opinion)*/\nagea /*Age of respondent, calculated*/\nhinctnta /*Household's total net income, all sources*/\nsmdfslv /*For fair society, differences in standard of living should be small*/\nsblazy /*Social benefits/services make people lazy*/\nsblwcoa /*Social benefits/services make people less willing care for one another*/\nsblwlka /*Social benefits/services make people less willing look after themselves/family*/\ntxautef /*Tax authorities, how efficient in doing their job*/\nditxssp /*Government decrease/increase taxes and social spending*/\ntxearn /*Taxation for higher versus lower earners*/\nearnpen /*Higher or lower earners should get larger old age pensions*/\nearnueb /*Higher or lower earners should get larger unemployment benefits*/\nlbenent /*Many with very low incomes get less benefit than legally entitled to*/\ninsfben /*Insufficient benefits in country to help people in real need*/\ntrstprl /*Trust in country's parliament*/\nedulvla /*Highest level of education*/\ntrstlgl /*Trust in the legal system*/\ntrstplt /*Trust in politicians*/\ntrstprt /*Trust in political parties*/\nimprich /*Important to be rich, have money and expensive things*/\nipeqopt /*Important that people are treated equally and have equal opportunities*/\nimpfree /*Important to make own decisions and be free*/\niphlppl /*Important to help people and care for others well-being*/\ngvjbevn /*Job for everyone, governments' responsibility*/\ngvhlthc /*Health care for the sick, governments' responsibility*/\ngvslvol /*Standard of living for the old, governments' responsibility*/\ngvslvue /*Standard of living for the unemployed, governments' responsibility*/\ngvcldcr /*Child care services for working parents, governments' responsibility*/\ngvpdlwk /*Paid leave from work to care for sick family, governments' responsibility*/;\nRUN;"
  },
  {
    "objectID": "projects/fiscalite/index.html#part-2-setup-of-the-variables",
    "href": "projects/fiscalite/index.html#part-2-setup-of-the-variables",
    "title": "Thesis on descriptive statistics on taxation in France",
    "section": "Part 2: Setup of the variables",
    "text": "Part 2: Setup of the variables\n\n\nAdd labels to the variables\n\nRenommage des variables avec des labels\ndata ess_label;\nset ess;\nlabel\ncntry = \"Pays de r√©sidence\"\nagea = \"Age\"\ngincdif = \"Le gouvernement doit prendre des mesures pour r√©duire les in√©galit√©s de revenu (1 = D'accord et 5 = Pas d'accord)\"\nhinctnta = \"Revenu\"\nsmdfslv = \"Pour une soci√©t√© juste, les diff√©rences de niveau de vie doivent √™tre faibles (1 = D'accord et 5 = Pas d'accord)\"\nsblazy = \"Les prestations sociales et les services sociaux rendent les personnes paresseuses (1 = Totalement d'accord et 5 = Totalement pas d'accord)\"\nsblwcoa = \"Les prestations sociales et les services sociaux conduisent les personnes √† moins prendre soin des autres (1 = Totalement d'accord et 5 = Totalement pas d'accord)\"\nsblwlka = \"Les prestations sociales et les services sociaux conduisent les personnes √† moins prendre soin de leurs proches (1 = Totalement d'accord et 5 = Totalement pas d'accord)\"\ntxautef = \"Les autorit√©s fiscales sont efficaces (10 = Tr√®s efficaces)\"\nditxssp = \"Il faut augmenter les imp√¥ts et les prestations sociales \"\ntxearn = \"Impots proportionnels (1), progressifs (2) ou √©gal (3) ?\"\nearnpen = \"Retraite en fonction du salaire (1 = Riche retraite plus √©lev√©e, 2 = Retraite √©gale et 3 = Riche retraite moins √©lev√©e\"\nearnueb = \"Les hauts salaires doivent-ils avoir une aide au ch√¥mage plus √©lev√©e (1), √©gale (2) ou moins √©lev√©e (3) ?\"\nlbenent = \"Beaucoup de personnes √† tr√®s faible revenu per√ßoivent moins d'aides que ce dont ils ont acc√®s\"\ntrstprl = \"Avez-vous confiance dans votre parlement ? (0 = Pas confiance et 10 = Confiance)\"\nedulvla = \"Niveau √©ducation maximal\"\ntrstlgl = \"Confiance dans le syst√®me l√©gal\"\ntrstplt = \"Confiance dans les politiciens\"\ntrstprt = \"Confiance dans les partis politiques\"\nimprich = \"Importance d'√™tre riche (1=Oui et 6=Non)\"\nipeqopt = \"Importance d'√™tre √©gaux (1=Oui et 6=Non)\"\nimpfree = \"Importance d'√™tre libre (1=Oui et 6=Non)\"\niphlppl = \"Importance d'aider les autres (1=Oui et 6=Non)\"\ngvjbevn = \"Responsabilit√© du gouvernement pour les emplois pour tous\"\ngvhlthc = \"Resp du gouv pour le syst√®me de sant√©\"\ngvslvol = \"Resp du gouv pour le niveau de vie des personnes √¢g√©es\"\ngvslvue = \"Resp du gouv pour le niveau de vie des ch√¥meurs\"\ngvcldcr = \"Resp du gouv pour le soin aux enfants de travailleurs\"\ngvpdlwk = \"Resp du gouv jour off pay√© afin de prendre soin famille malade\";\nRUN;\n\n\n\nAdd formats to the variables\n\nCr√©ation de formats\n\nPROC FORMAT;\nvalue income\n1-5 = \"Classe populaire et moyenne inf√©rieure\"\n6-8 = \"Classe moyenne\"\n9-10 = \"Classe sup√©rieure\"\n77 = \"A refus√© de r√©pondre\"\n88 = \"Ne sais pas\"\n99 = \"Pas de r√©ponse\";\nrun;\n\nPROC FORMAT;\nvalue age\n0-&lt;15 = \"Enfant ou adolescent\"\n15-&lt;30 = \"Moins de 30 ans\"\n30-65 = \"Entre 30 et 65 ans\"\n65-500 = \"Plus de 65 ans\"\n999 = \"Pas de r√©ponse\";\nRUN;\n\nPROC FORMAT;\nvalue $country\n\"DK\",\"SE\",\"NO\",\"FI\" = \"Mod√®le Social-D√©mocrate\"\n\"FR\",\"BE\",\"LU\",\"DE\",\"AT\" = \"Mod√®le Conservateur/Corporatiste\"\n\"GB\",\"CY\",\"IE\"=\"Mod√®le Lib√©ral\"\n\"AL\",\"HR\",\"ME\",\"MK\",\"SI\",\"XK\",\"SK\",\"CH\",\"NL\",\"GE\",\"IL\",\"TR\",\"EE\",\"IS\",\"RO\",\"RS\",\"BG\",\n\"CZ\",\"HU\",\"PL\",\"RU\",\"UA\",\"LV\",\"LT\",\"ES\",\"IT\",\"GR\",\"PT\" = \"Autre\";\nRUN;\n\nPROC FORMAT;\nvalue presta_sociale\n1 = \" Absolument d'accord\"\n2 = \"Plut√¥t d'accord\"\n3 = \"Indiff√©rent\"\n4 = \"Plut√¥t pas d'accord\"\n5 = \"Absolument pas d'accord\"\n7 = \"A refus√© de r√©pondre\"\n8 = \"Ne sais pas\"\n9 = \"Pas de r√©ponse\";\nrun;\n\nPROC FORMAT;\nvalue resp_gouv\n0-2 = \"Absolument pas d'accord\"\n3-4 = \"Plut√¥t pas d'accord\"\n5 = \"Sans opinion\"\n6-7 = \"Plut√¥t d'accord\"\n8-10 = \"Absolument d'accord\"\n77 = \"A refus√© de r√©pondre\"\n88 = \"Ne sais pas\"\n99 = \"Pas de r√©ponse\";\nRUN;\n\nPROC FORMAT;\nvalue gouv_evolution_impot_ps\n0-2 = \"Absolument pas d'accord\"\n3-4 = \"Plut√¥t pas d'accord\"\n5 = \"Sans opinion\"\n6-7 = \"Plut√¥t d'accord\"\n8-10 = \"Absolument d'accord\"\n77 = \"A refus√© de r√©pondre\"\n88 = \"Ne sais pas\"\n99 = \"Pas de r√©ponse\";\nRUN;\n\nPROC FORMAT;\nvalue aides\n1 = \"Absolument d'accord\"\n2 = \"Plut√¥t d'accord\"\n3 = \"Indiff√©rent\"\n4 = \"Plut√¥t pas d'accord\"\n5 = \"Absolument pas d'accord\"\n7 = \"A refus√© de r√©pondre\"\n8 = \"Ne sais pas\"\n9 = \"Pas de r√©ponse\";\nRUN;\n\nPROC FORMAT;\nvalue comment_taxer\n1 = \"Imp√¥t proportionnel\"\n2 = \"Imp√¥t progressif\"\n3 = \"Imp√¥t √©gal\"\n4 = \"Aucun de ces r√©ponse\"\n7 = \"A refus√© de r√©pondre\"\n8 = \"Ne sais pas\"\n9 = \"Pas de r√©ponse\";\nRUN;\n\nPROC FORMAT;\nvalue education_ISCED\n0 = \"Impossible d'harmoniser\"\n1 = \"Moins que secondaire\"\n2-3 = \"Secondaire\"\n4-5 = \"Sup√©rieur\"\n55 = \"Autre\"\n77 = \"A refus√© de r√©pondre\"\n88 = \"Ne sais pas\"\n99 = \"Pas de r√©ponse\";\nRUN;\n\nPROC FORMAT;\nvalue gouv\n0-3 = \"Pas d'accord\"\n4-6 = \"Sans opinion\"\n7-10 = \"D'accord\"\n77 = \"A refus√© de r√©pondre\"\n88 = \"Ne sais pas\"\n99 = \"Pas de r√©ponse\";\nRUN;\n\nPROC FORMAT;\nvalue imp\n0-2 = \"Important\"\n2-4 = \"Neutre\"\n4-6 = \"Pas important\"\n7 = \"A refus√© de r√©pondre\"\n8 = \"Ne sais pas\"\n9 = \"Pas de r√©ponse\";\nRUN;"
  },
  {
    "objectID": "projects/fiscalite/index.html#part-3-make-the-plots",
    "href": "projects/fiscalite/index.html#part-3-make-the-plots",
    "title": "Thesis on descriptive statistics on taxation in France",
    "section": "Part 3: Make the plots",
    "text": "Part 3: Make the plots\n\n\nCreate function for the plots\n\nCr√©ation des macros que l‚Äôon va utiliser\n\n%MACRO panel_var1_var2(variable1,variable2,format1,format2,maxvar1,maxvar2);/*Cette macro cr√©e un panneau suivant les valeurs d'une variable \"variable1\" cod√©e par le format \"format1\" et dans chaque case du panneau repr√©sente les proportions des valeurs de la variable \"variable2\" cod√©e par le format \"format2\"*/\n\n  /*Tri de la table suivant la variable \"variable1\"*/\n  proc sort data=ess_label;\n    by &variable1;\n  run;\n  /*Calcul des pourcentages de chaque valeur de la variable \"variable2\" en groupant selon les valeurs de la variable \"variable1\"*/\n  proc freq data=ess_label\n  (WHERE = (cntry=\"FR\" and &variable1&lt;=&maxvar1 and &variable2&lt;=&maxvar2))\n  noprint;\n    format &variable1 %scan(&format1, 1, %str(%bquote(%')%str(%\"))).;\n    by &variable1;\n    tables &variable2 / out=FreqOut;\n  run;\n  \n  /*Cr√©ation du panneau suivant les valeurs de la variable \"variable1\" en repr√©sentant les proportions des valeurs de la variable \"variable2\" */\n  proc sgpanel data=FreqOut;\n    format &variable1 %scan(&format1, 1, %str(%bquote(%')%str(%\"))).;\n    format &variable2 %scan(&format2, 1, %str(%bquote(%')%str(%\"))).;\n    styleattrs datacontrastcolors=(red green blue) datacolors=(red green blue);\n    panelby &variable1 / columns = 3 novarname noheader;\n    vbar &variable2 /group=&variable1 barwidth=0.5\n  response=percent datalabel ;\n  run;\n  \n%MEND;\n\n\n%MACRO barres_stack(variable);/*Cette macro permet pour une variable \"variable\", qui sera une variable de responsabilit√© du gouvernement, de faire un histogramme horizontal avec barres empil√©es pour repr√©senter cette variable pour les diff√©rentes classes de revenu*/\n\n  /*Tri de la table suivant le revenu avec les observations utiles*/\n  proc sort data=ess_label (WHERE=(cntry= \"FR\" and hinctnta&lt;=10 and &variable&lt;=10 )) out=ess_label_bis ;\n    format hinctnta income.;\n    by hinctnta;\n  run;\n  \n  /*Calcul des pourcentages de chaque valeur de la variable \"variable\" en groupant selon les classes de revenu*/\n\n  proc freq data=ess_label_bis noprint;\n    by hinctnta;\n    tables &variable / out=FreqOut;\n  run;\n  \n  /*Cr√©ation de l'histogramme horizontal avec barres empil√©es*/\n  proc sgplot data=FreqOut NOAUTOLEGEND;\n    styleattrs datacontrastcolors=(red green blue) datacolors=(red green blue);\n    format hinctnta income.;\n    format &variable gouv.;\n    hbar hinctnta / response=Percent group=&variable groupdisplay=stack barwidth=0.5 ;\n    xaxis grid values=(0 to 100 by 10);\n  run;\n  \n%MEND;\n\n\n%MACRO trust(variable1,variable2,maxvar2); /*Cette macro √©tudie le lien entre diff√©rentes variables de confiances \"variable1\" en abscisse et \"variable2\" en ordonn√©es. Elle fait une moyenne sur les pays et repr√©sente une droite de tendance*/\n\n  /*Calcule la moyenne de la variable \"variable1\" pour chaque pays*/\n  proc means data=ess_label (where = (&variable2 &lt;=&maxvar2 and &variable1&lt;50)) noprint;\n    class cntry;\n    var &variable1 &variable2 ;\n    output out=mean_data_2 mean=;\n  run;\n  /*Repr√©sente en abscisses la variable \"variable1\" et en ordonn√©e \"variable2\" avec la droite de tendance*/\n  proc sgplot data=mean_data_2;\n    scatter x=&variable1 y=&variable2 / group=cntry;\n    keylegend / location=outside;\n    reg x=&variable1 y=&variable2 / lineattrs=(color=red) NOMARKERS;\n  run;\n%MEND;\n\n\n%MACRO consensus(variable, format); /*Cette macro √©tudie les fr√©quences de la variable \"variable\" cod√©e par le format \"format\" pour voir s'il existe un consensus aupr√®s des individus*/\n\n  /*Calcul des fr√©quences des valeurs de la variable \"variable\"*/\n  proc freq data=ess_label (WHERE=(cntry=\"FR\" /*and 8&lt;=hinctnta&lt;=10 Pour le dernier consensus */ ));\n    tables %scan(&variable, 1, %str(%bquote(%')%str(%\"))) / out=FreqOut;\n  run;\n  \n/*Cr√©ation del'histogramme pour voir s'il existe un consensus*/\n  proc sgplot data=FreqOut NOAUTOLEGEND;\n    styleattrs datacontrastcolors=(red orange green blue purple red) datacolors=(red orange green blue purple red);\n    VBAR %scan(&variable, 1, %str(%bquote(%')%str(%\"))) /response=percent group=%scan(&variable, 1, %str(%bquote(%')%str(%\")))\n    datalabel; format %scan(&variable, 1, %str(%bquote(%')%str(%\")))\n%scan(&format, 1, %str(%bquote(%')%str(%\"))).;\n  run;\n  \n%MEND;\n\n%MACRO vari_en_fct_pays(variable);/*Cette macro cr√©e un graphique de points avec en abscisse la cat√©gorie de pays (Europe Centrale, Europe du Nord, Lib√©ral, Familialiste) et en ordonn√©e la moyenne de la variable \"variable\" pour chaque pays de chaque cat√©gorie*/\n\n  /*Renvoit la moyenne de la variable \"variable\" pour chaque pays pr√©sent dans la table */\n  proc means data=ess_label (where = (&variable&lt;=4)) noprint;\n    class cntry;\n    var &variable;\n    output out=mean_data mean=;\n  run;\n  /*Selectionne les moyennes qui correspondent aux pays des diff√©rentes cat√©gories √©tudi√©es*/\n  data mean_data;\n    set mean_data;\n    if cntry in (\"DK\",\"SE\",\"NO\",\"FI\",\"FR\",\"BE\",\"LU\", \"DE\",\"AT\",\"GB\",\"CY\",\"IE\");\n  run;\n/*Cr√©ation du graphique de points en regroupant par la cat√©gorie de pays afin d'obtenir des couleurs diff√©rentes*/\n  proc sgplot data=mean_data NOAUTOLEGEND;\n    styleattrs datacontrastcolors=(red green blue) datacolors=(red green blue);\n    format cntry $country.;\n    scatter x=cntry y=&variable /group=cntry;\n  run;\n  \n%MEND;\n\n\n\nUsed plots\n\nGraphiques utilis√©s\n\n/*I.A.1*/\n    %panel_var1_var2(hinctnta,ditxssp,\"income\",\"gouv_evolution_impot_ps\",10,10);\n    %panel_var1_var2(hinctnta,sblazy,\"income\",\"presta_sociale\",10,5);\n    %panel_var1_var2(hinctnta,lbenent,\"income\",\"aides\",10,5);\n    %panel_var1_var2(hinctnta,insfben,\"income\",\"aides\",10,5);\n    \n/*I.A.2*/\n    %barres_stack(gvjbevn);\n    %barres_stack(gvhlthc);\n    %barres_stack(gvslvol);\n    %barres_stack(gvslvue);\n    %barres_stack(gvcldcr);\n    %barres_stack(gvpdlwk);\n    \n/*I.B*/\n    /*Acceptation*/\n        %trust(trstprl,ditxssp,10);\n        %trust(trstlgl,ditxssp,10);\n        %trust(trstplt,ditxssp,10);\n        %trust(trstprt,ditxssp,10);\n    /*M√©thode*/\n        %trust(trstprl,txearn,3);\n        %trust(trstlgl,txearn,3);\n        %trust(trstplt,txearn,3);\n        %trust(trstprt,txearn,3);\n    /*Efficacit√©*/\n        %trust(trstprl,txautef,10);\n        %trust(trstlgl,txautef,10);\n        %trust(trstplt,txautef,10);\n        %trust(trstprt,txautef,10);\n\n/*I.C*/\n    %consensus(\"gincdif\",\"presta_sociale\");\n    %consensus(\"sblwcoa\",\"presta_sociale\");\n    %consensus(\"txearn\",\"comment_taxer\");\n    %consensus(\"ipeqopt\",\"imp\");\n    %consensus(\"smdfslv\",\"presta_sociale\");\n    \n/*II.A*/\n    %panel_var1_var2(hinctnta,ditxssp,\"income\", \"gouv_evolution_impot_ps\",10,10);\n    \n/*II.B*/\n  %panel_var1_var2(edulvla,ditxssp,\"education_ISCED\", \"gouv_evolution_impot_ps\",10,10);\n\n/*II.C*/\n    %panel_var1_var2(agea,ditxssp,\"age\", \"gouv_evolution_impot_ps\",200,10);\n    \n/*III*/\n    %vari_en_fct_pays(imprich);\n    %vari_en_fct_pays(ipeqopt);\n    %vari_en_fct_pays(impfree);\n    %vari_en_fct_pays(iphlppl);\n    %vari_en_fct_pays(earnpen);\n    %vari_en_fct_pays(earnueb);\n    %vari_en_fct_pays(sblwlka);\n    %vari_en_fct_pays(sblazy);"
  },
  {
    "objectID": "interview_questions/project1/index.html",
    "href": "interview_questions/project1/index.html",
    "title": "Brainteasers",
    "section": "",
    "text": "Let‚Äôs discover some interview questions labelled as brain teasers\n\n\n\n\n\n\n\n\n\n\n\n\nBrainteaser 1\n\n\n\n\n\n\nMihnea POPA\n\n\n\n\n\n\n\n\n\n\n\nBrainteaser 2\n\n\n\n\n\n\nMihnea POPA\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "interview_questions/project1/index.html#sub-header",
    "href": "interview_questions/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "interview_questions.html",
    "href": "interview_questions.html",
    "title": "My Interview questions",
    "section": "",
    "text": "Brainteasers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbabilities\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "interview_questions/project2/index.html",
    "href": "interview_questions/project2/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let‚Äôs investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of -0.85.\n\n\nHere is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "interview_questions/project2/index.html#sub-header",
    "href": "interview_questions/project2/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "interview_questions/project1/brainteaser/2/index.html",
    "href": "interview_questions/project1/brainteaser/2/index.html",
    "title": "Brainteaser 2",
    "section": "",
    "text": "Second problem"
  },
  {
    "objectID": "interview_questions/project1/brainteaser/1/index.html",
    "href": "interview_questions/project1/brainteaser/1/index.html",
    "title": "Brainteaser 1",
    "section": "",
    "text": "First problem"
  },
  {
    "objectID": "interview_questions/brainteasers/brainteaser/2/index.html",
    "href": "interview_questions/brainteasers/brainteaser/2/index.html",
    "title": "Brainteaser 2",
    "section": "",
    "text": "Second problem"
  },
  {
    "objectID": "interview_questions/brainteasers/brainteaser/1/index.html",
    "href": "interview_questions/brainteasers/brainteaser/1/index.html",
    "title": "Brainteaser 1",
    "section": "",
    "text": "First problem"
  },
  {
    "objectID": "interview_questions/brainteasers/index.html",
    "href": "interview_questions/brainteasers/index.html",
    "title": "Brainteasers",
    "section": "",
    "text": "Let‚Äôs discover some interview questions labelled as brain teasers\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinding a lighter coin with minimum weightings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasuring time with burning ropes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinding the fastest horses\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAvoiding prisoners escape\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDetermining the angle at 3:15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCube explosion\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFallen eggs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisened Wine\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWind effect on a plane\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating 2 groups with equal heads\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCounting squares in a grid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCrossing a bridge in minimum time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputing ratio speed on a circular track\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrisoners random hat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAverage Speed of a train travel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWine in water or water in wine ?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolyglots in society\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCatching a furtive spy\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "interview_questions/brainteasers/2.html",
    "href": "interview_questions/brainteasers/2.html",
    "title": "Brainteaser 2",
    "section": "",
    "text": "Second problem"
  },
  {
    "objectID": "interview_questions/brainteasers/1.html",
    "href": "interview_questions/brainteasers/1.html",
    "title": "Measuring time with burning ropes",
    "section": "",
    "text": "Let‚Äôs suppose that we have two ropes and each takes one hour to burn but not uniformly. How can we measure 45 minutes ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nTry to think about everything that you can do with the ropes and the lighter.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nAt \\(t=0 \\text{min}\\), we burn one end of the first rope and the two ends of the second rope\n\\[\n\\begin{aligned}\n&üî• \\rule{2cm}{0.4pt} \\\\\n&üî• \\rule{2cm}{0.4pt}üî•\n\\end{aligned}\n\\] After a half-hour, so at \\(t=30 \\text{min}\\), the second rope has entirely burned and the first one has a remaining of 30 minutes. At this time, we burn the second end of the first rope.\n\\[\n\\begin{aligned}\n& \\rule{0.5cm}{0.01pt}üî• \\rule{1.5cm}{0.4pt}üî• \\\\\n& \\rule{0.5cm}{0.01pt}üî• \\rule{0cm}{0.4pt}üî•\n\\end{aligned}\n\\] Finally, the remaining part of the first rope takes 30 minutes to burn but if we burn at both ends, it only takes 15 minutes. Therefore at \\(t=45 \\text{min}\\), the first rope will also be burned entirely. \\[\n\\begin{aligned}\n& \\rule{1.25cm}{0.01pt}üî• \\rule{0.75cm}{0.4pt}üî• \\\\\n& \\rule{0.5cm}{0.01pt}üî• \\rule{0cm}{0.4pt}üî•\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "interview_questions/probabilities/2.html",
    "href": "interview_questions/probabilities/2.html",
    "title": "Brainteaser 2",
    "section": "",
    "text": "Second problem"
  },
  {
    "objectID": "interview_questions/probabilities/1.html",
    "href": "interview_questions/probabilities/1.html",
    "title": "Brainteaser 1",
    "section": "",
    "text": "First problem"
  },
  {
    "objectID": "interview_questions/probabilities/index.html",
    "href": "interview_questions/probabilities/index.html",
    "title": "Probabilities",
    "section": "",
    "text": "Let‚Äôs discover some interview questions labelled as probabilities\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocal minima in a permutation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFake coin probability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKnight of Mere\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModes of a Poisson distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaximum of probability equality two Bernoulli\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParity of the sum of dices\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLost key in the last classroom\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDrunk postman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolya‚Äôs urn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperation on Uniform distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExpected coin flips until different result\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom boxes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTriangle from a broken stick\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "interview_questions/brainteasers/time_burning_ropes.html",
    "href": "interview_questions/brainteasers/time_burning_ropes.html",
    "title": "Measuring time with burning ropes",
    "section": "",
    "text": "Let‚Äôs suppose that we have two ropes and each takes one hour to burn but not uniformly. How can we measure 45 minutes ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nTry to think about everything that you can do with the ropes and the lighter.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nAt \\(t=0 \\text{min}\\), we burn one end of the first rope and the two ends of the second rope\n\\[\n\\begin{aligned}\n&üî• \\rule{2cm}{0.4pt} \\\\\n&üî• \\rule{2cm}{0.4pt}üî•\n\\end{aligned}\n\\] After a half-hour, so at \\(t=30 \\text{min}\\), the second rope has entirely burned and the first one has a remaining of 30 minutes. At this time, we burn the second end of the first rope.\n\\[\n\\begin{aligned}\n& \\rule{0.5cm}{0.01pt}üî• \\rule{1.5cm}{0.4pt}üî• \\\\\n& \\rule{0.5cm}{0.01pt}üî• \\rule{0cm}{0.4pt}üî•\n\\end{aligned}\n\\] Finally, the remaining part of the first rope takes 30 minutes to burn but if we burn at both ends, it only takes 15 minutes. Therefore at \\(t=45 \\text{min}\\), the first rope will also be burned entirely. \\[\n\\begin{aligned}\n& \\rule{1.25cm}{0.01pt}üî• \\rule{0cm}{0.4pt}üî• \\\\\n& \\rule{0.5cm}{0.01pt}üî•  \\rule{0cm}{0.4pt}üî•\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "interview_questions/brainteasers/unique_ligher_coin.html",
    "href": "interview_questions/brainteasers/unique_ligher_coin.html",
    "title": "Finding a ligher coin with minimum weightings",
    "section": "",
    "text": "Let‚Äôs suppose that we have 100 coins but one is ligher that the others. Find the minimum number of weightings to identify this fake coin.\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nTry to think about how many outcomes can one weighting differentiate.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nEach weighing can lead to three different outcomes:\n\nThe left part is heavier\nThe left part is lighter\nThe left part and the right part have same weight\n\nNow we can find the strategy:\n\nFirst weighting : 33 vs 33 (34 left)\nSecond weighting:\n\nIf one of the groups was lighter: 11 vs 11 (11 left)\nIf same weight: 11 vs 11 (12 left)\n\nThird weighting:\n\nIf the second case led to same weight : 4 vs 4 (4 left)\nIf not: 4 vs 4 (3 left)\n\nFourth weighting:\n\nIf the second case led to same weight: 1 vs 1 (1 left) ‚úÖ\nIf not : 1 vs 1 (2 left)\n\nFifth weighting:\n\nIf the second case led to different weights: ‚úÖ\nIf not : 1 vs 1 (0 left) ‚úÖ\n\n\nSo the minimum number of weighting to be sure to identify the fake coin in n = 5.\n\n\n\n\n\n\n\n\n\nTipüö® Generalisation\n\n\n\n\n\nWe can try to generalise this strategy. Let‚Äôs assume that there are \\(N\\) coins with one fake. Then since each weighting leads to \\(3\\) different outcomes, we can identify the fake coins with \\(n\\) weightings if \\(3^n&gt;N\\). We want the minimum number of weightings to be sure to identify the coin, thus \\[3^{n-1}&lt;N \\le 3^n.\\] Accordingly, we have \\[n = \\lfloor \\frac {ln(N)}{ln(3)} \\rfloor + 1 .\\] With \\(N=100\\), we find indeed \\(n=5\\)."
  },
  {
    "objectID": "interview_questions/brainteasers/horse_race.html",
    "href": "interview_questions/brainteasers/horse_race.html",
    "title": "Finding the fastest horses",
    "section": "",
    "text": "Let‚Äôs suppose that we have 25 horses and we can race 5 horses at a time. What is the minimum number of races to identify the 3 fastest horses ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nWe can put them in five groups of 5 horses and within a group we will know the order. But how can we order them across the groups ?\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nWe start by separating the horses into five different groups and we run a race in each group. By symmetry we can denote by A, B, C, D and E the groups and by the indexes corresponding to their order. Therefore, we have \\[\n\\begin{aligned}\n& A_1 &lt; A_2 &lt; A_3 &lt; A_4 &lt; A_5 \\\\\n& B_1 &lt; B_2 &lt; B_3 &lt; B_4 &lt; B_5 \\\\\n& C_1 &lt; C_2 &lt; C_3 &lt; C_4 &lt; C_5 \\\\\n& D_1 &lt; D_2 &lt; D_3 &lt; D_4 &lt; D_5 \\\\\n& E_1 &lt; E_2 &lt; E_3 &lt; E_4 &lt; E_5. \\\\\n\\end{aligned}\n\\] Now we will run a new race with the champions of each group and by symmetry, we can assume \\(A_1 &lt; B_1 &lt; C_1 &lt; D_1 &lt; E_1.\\) And now, for each horse we find whether or not he could be in the 3 fastest. For instance, the horse \\(D_1\\) is slower that \\(A_1, B_1, C_1\\), so he cannot be in the three fastest. Likewise, the horse \\(B_3\\) is slower that \\(A_1, B_1, B_2\\), so he cannot be in the three fastest. However, the horse \\(B_2\\) is only slower that \\(A_1, B_1\\), so he is a possible candidate. By doing this for each horse, we find that the possible candidates are \\(A_1, A_2, A_3, B_1, B_2, C_1.\\)\n\\[\n\\begin{aligned}\n& A_1 &lt; A_2 &lt; A_3 &lt; \\cancel{A_4} &lt; \\cancel{A_5} \\\\\n& B_1 &lt; B_2 &lt; \\cancel{B_3} &lt; \\cancel{B_4} &lt; \\cancel{B_5} \\\\\n& C_1 &lt; \\cancel{C_2} &lt; \\cancel{C_3} &lt; \\cancel{C_4} &lt; \\cancel{C_5} \\\\\n& \\cancel{D_1} &lt; \\cancel{D_2} &lt; \\cancel{D_3} &lt; \\cancel{D_4} &lt; \\cancel{D_5} \\\\\n& \\cancel{E_1} &lt; \\cancel{E_2} &lt; \\cancel{E_3} &lt; \\cancel{E_4} &lt; \\cancel{E_5}. \\\\\n\\end{aligned}\n\\] Now, we only have six possible candidates but we already know that \\(A_1\\) is the fastest. So we only need to race \\(A_2, A_3, B_1, B_2, C_1\\) and take the two fastest ones.\nAccordingly, the minimum number of races to identify the three fastest horses is \\(\\boxed{5+ 1 + 1  = 7.}\\)"
  },
  {
    "objectID": "interview_questions/brainteasers/one_bullet_more_prisoners.html",
    "href": "interview_questions/brainteasers/one_bullet_more_prisoners.html",
    "title": "Avoiding prisoners escape",
    "section": "",
    "text": "Let‚Äôs suppose that we are guarding a prison with 100 prisoners each with a probability p_i to try escaping (everyone knows every probability). Assuming that we only have 1 bullet, what is our strategy to be sure that no prisoner will try to escape ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nEach prisoner would rather live than try to escape if he surely dies trying. Thus, the strategy uses recursion that the previous inmate will not try escaping to prove that the next one will not try either.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nLet‚Äôs order our prisoners by their probability of trying to escape \\(p_1 &lt; p_2 &lt;...&lt;p_n.\\) This optimal strategy consists in saying I will kill the inmate with the lowest probability among those who try to escape. Therefore, the prisoner 1 will not try because he knows that he has the lowest probability and if he tries then he will die. Then, the prisoner 2 knows that the first one will not try and so he knows that if he tries, we will be the one with the lowest probability. This reasoning works for every prisoner, so no one will try to escape.\nNB: This also works if we take the highest.\n\n\n\n\n\n\n\n\n\nTipüö® Generalisation\n\n\n\n\n\nThe strategy showed before only works if we can order the probabilities strictly. Indeed, if two inmates have the same probability then the guard will not know which one to shoot since he has only one bullet. Therefore, if they try, they do not die surely (if 2 people have the same probability then there is a chance of 1/2 to survive). Accordingly, the guard will need more bullets. In fact, if there is a group with 2 people with the same probability and another with 3 people with the same probability, he will need 3 bullets to be sure that no one tries to escape. Therefore, if we suppose that there are N prisoners then we have, \\(\\boxed{n_{bullets} = \\text{max}_{1\\le i \\le n} \\; Card(\\{j \\in \\lbrack\\!\\lbrack 1,N \\rbrack\\!\\rbrack| \\; p_j = p_i\\}).}\\)"
  },
  {
    "objectID": "interview_questions/brainteasers/clock_angle.html",
    "href": "interview_questions/brainteasers/clock_angle.html",
    "title": "Determining the angle at 3:15",
    "section": "",
    "text": "Let‚Äôs suppose that a clock shows 3:15, what is the angle between the minute hand and the hour hand ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nBetween 3:00 and 3:15, the hour hand had traveled a bit but how far did it go ?\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nAfter 15 minutes, the minutes hand had made a quarter of a turn so the angle with the x:00 axis is 90¬∞.\nThe hour hand made an angle of 90¬∞ at 3:00. One hour after, at 4:00, it should be at \\(\\frac {4}{12}*360 = 120¬∞.\\) So, in one hour, this hand at turned by 30¬∞ (which means \\(\\frac{30}{4} = 7.5¬∞\\) each quarter hour).\nThus, at 3:15, the angle between the two hands is 7.5¬∞.\n\n\n\n\n\n\n\n\n\n\nTipüö® Generalisation\n\n\n\n\n\nThis generalisation is more like a follow-up question where we have to compute the exact time after 3:00 when the two hands overlap.\nTo compute this, we need the angular speed of each hands: * For the minute one, it takes 60 minutes to do one turn (360¬∞) so we have \\(v_{min} = \\frac{360}{60} = 6 ¬∞/min.\\) * For the hour one, it takes 12 hours to do one turn so we have \\(v_{hour} = \\frac{360}{12*60} = 0.5 ¬∞/min.\\)\nAt 3:00, the initial angle of the minutes hand is 0¬∞ and the initial angle of the hours hand is 90¬∞. Thus we are trying to solve the equation \\(0 + 6*t = 90 + 0.5*t.\\) We find \\(\\boxed{t=\\frac{180}{11} = 16.37 \\;min = 16 \\; min \\; 22 \\; s}\\)"
  },
  {
    "objectID": "interview_questions/brainteasers/fallen_eggs.html",
    "href": "interview_questions/brainteasers/fallen_eggs.html",
    "title": "Fallen eggs",
    "section": "",
    "text": "We have a building with 100 floors and 2 eggs. What is the optimal strategy in order to find the floor where the eggs break by minimizing the worst case scenario ?\n\n\n\n\n\n\nTipüí° Hint (1)\n\n\n\n\n\nIf we try the floor n for the first drop and it breaks then the worst case scenario for the second is to try every floor from 1 to n-1, so the total number of drops would be n.\n\n\n\n\n\n\n\n\n\nTipüí° Hint (2)\n\n\n\n\n\nIf we try the floor n for the first drop and it does not break and then we try the floor 2n and it breaks then the worst case scenario for the second egg is to try every floor from n+1 to 2n-1, so the total number of drops would be n+1. So we need a strategy where for each successful drop of the first egg, we reduce by one unit the worst-case scenario for the second one.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nNaively, if we try a drop at the floor 50 (100/2) and it breaks then in the worst-case scenario we have to try all floors from 1 to 49 so we would need 49 drops for the second egg (in that case, the floor where the egg starts to break is either 49 or 50). So the worst-case scenario would be \\(WC_{\\text{half}} = 1 + 49 = 50 \\; \\text{drops}.\\)\nThe idea is that if the egg breaks at 50 then we don‚Äôt have enough information, thus let‚Äôs try the floor 10 and add 10 each time. If the egg breaks at 50 (5 drops of the first egg to get there), then in the worst-case scenario we have to try all floors from 41 to 49 so we would need 9 additional drops. Accordingly, \\(WC_{\\text{constant}} = 5 + 9 = 14 &lt; WC_{\\text{half}}.\\) We already have a better strategy than the first one. However, if instead of breaking at 50, the first egg breaks at 90, we would still need to test 9 values in the worst-case scenario (81, ‚Ä¶, 89). So the total number of drops will raise to 18. Therefore, we need a strategy where every successful drop of the first egg, leeds to a smaller uncertainty of the second egg by one unit.\nThis optimal strategy would be that at the first drop we try the floor \\(n\\). If it does not succeed then in the worst scenario, we would add n-1 drops (1,2, ‚Ä¶, n-1) so the total amount would be n.¬†If the first drop is a success then we try the floor \\(n + (n-1) = 2n-1\\). If this one does not succeed then in the worst scenario, we would add n-2 drops (n+1, ‚Ä¶, 2n-2) so the total will remain n.¬†Accordingly, we need to find n such that \\(n + (n-1) + (n-2) + ... + 1\\ge100\\) (the sum needs to end at 1 because let‚Äôs say we are at 97 and we try 99 and it fails we don‚Äôt know if it breaks at 98 or 99). The condition is the same as \\(n(n+1)\\ge200.\\) By testing, a few values we find \\(\\boxed{n_{\\text{optimal}} = 14.}\\)\nTherefore, we are trying the floors 14, 27, 39, 50, 60, 69, 77, 84, 90, 95, 99 and the number of total drops in the worst-case scenario is always 14."
  },
  {
    "objectID": "interview_questions/brainteasers/cube_explosion.html",
    "href": "interview_questions/brainteasers/cube_explosion.html",
    "title": "Cube explosion",
    "section": "",
    "text": "Let‚Äôs suppose that we paint a cube on all of its faces. Then we decompose this cube into 27 small cubes. How many small cubes has two faces painted ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nFor every possible number of painted faces, try to count the number of small cubes with that number of painted faces.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nOne small cube cannot have more that 3 faces painted so we will count how many small cube have 0, 1, 2 and 3 painted faces.\n\n\n\n\n\n\n\n\nNumber of painted faces\nNumber of cubes\nExplanation\n\n\n\n\n0\n1\nCenter of the original cube\n\n\n1\n6\nCenter of each face of the original cube\n\n\n3\n8\nAll the vertices of the original cube\n\n\n2\n12\nOn the edge of every face but not on the vertices of the cube"
  },
  {
    "objectID": "interview_questions/brainteasers/poisened_wine.html",
    "href": "interview_questions/brainteasers/poisened_wine.html",
    "title": "Poisened Wine",
    "section": "",
    "text": "Let‚Äôs suppose that we have 1000 bottle of wine but one is poisened. Knowing that the poison takes 24h to be effective and that we need to know which bottle is poisoned in 24 hours, what is the minimum number of mice to find the poisened bottle ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nOne mouse can drink from multiple wines (make a mix).\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nEach mouse can either drink from a specific bottle of wine or not, thus, there are 2 outcomes. If there are \\(n\\) mice, then the total number of outcome covered is \\(2^n\\). Since, we want the minimum number of mice, we have \\(2^{n-1}&lt;1000\\le 2^n\\), thus, \\(\\boxed{n=\\lfloor \\frac{ln(1000)}{ln(2)} \\rfloor} + 1 = 10.\\)"
  },
  {
    "objectID": "interview_questions/brainteasers/groups_equal_heads.html",
    "href": "interview_questions/brainteasers/groups_equal_heads.html",
    "title": "Creating 2 groups with equal heads",
    "section": "",
    "text": "Imagine that we have 100 coins and among them there are 20 heads. Create two groups of coins with the same number of heads.\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nWhat can we do with those coins ? We can only regroup them and flip them.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nMake a group with 20 coins (k Heads unknown), therefore on the remaining 80 there are 20-k Heads. Now just flip each coin of the first group. Now, both groups have exactly the same number of coins. We can prove this result more generally in the next section.\n\n\n\n\n\n\n\n\n\nTipüö® Generalisation\n\n\n\n\n\nLet \\(N\\) be the total number of coins and \\(n\\) the number of heads. We will make a group with \\(m\\) coins and try to find the value of m.\n\n\n\nGroup 1\nGroup 2\n\n\n\n\nNumber coins: m\nN - m\n\n\nNumber heads: k\nn - k\n\n\nNumber tails: m-k\nN-m-n+k\n\n\n\nIf we flip the first group\n\n\n\nGroup 1\nGroup 2\n\n\n\n\nNumber coins: m\nN - m\n\n\nNumber heads: m-k\nn - k\n\n\nNumber tails: k\nN-m-n+k\n\n\n\nWe want \\(m - k = n-k\\) thus \\(\\boxed{m=n.}\\)\nIf we flip the second group\n\n\n\nGroup 1\nGroup 2\n\n\n\n\nNumber coins: m\nN - m\n\n\nNumber heads: k\nN-m-n+k\n\n\nNumber tails: m-k\nn-k\n\n\n\nWe want \\(k=N-m-n+k\\) thus \\(\\boxed{m=N-n.}\\)\nFinally, we find the same solution, we just always have to flip the small group (it is logical since if we flip the big group we would have more heads than 20)"
  },
  {
    "objectID": "interview_questions/brainteasers/average_speed.html",
    "href": "interview_questions/brainteasers/average_speed.html",
    "title": "Average Speed of a train travel",
    "section": "",
    "text": "A train travels at 60 mph on the trip from A to B and 40 mph on the trip from B to A. What is the average speed ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nThis should be pretty straightforward when you introduce the good notations. I can only tell that the average speed is not the average of both speeds (the first trip is faster and will contribute less to the total time).\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nLet \\(v_1 = 60 \\; mph\\) and \\(v_2 = 40 \\; mph\\). Moreover, we introduce the distance \\(d\\) between the points A et B.\nThen, the average speed follows \\(\\bar{v} = \\frac{d+d}{t_1 + t_2}\\) where \\(t_1 = \\frac{d}{v_1}\\) and \\(t_2 = \\frac{d}{v_2}\\). Thus we have \\(\\boxed{\\bar{v}=\\frac{2}{1/v_1 +1/v_2}.}\\) We find \\(\\bar{v}= 48 \\; mph\\)\n\n\n\n\n\n\n\n\n\nTipüö® Average speed paradox\n\n\n\n\n\nLet‚Äôs assume that a runner does a lap at a speed v. At what speed does he have to run on the second lap to have an average speed of 2v ?\nThe answers lays in the fact that we want \\(\\bar{v}=2*v_1\\). Accordingly, the formula seen before leads to \\(2v_1 = \\frac{2}{1/v_1 +1/v_2}\\), the only solution would be that \\(\\boxed{v_2 \\rightarrow + \\infty.}\\)"
  },
  {
    "objectID": "interview_questions/brainteasers/wind_effect.html",
    "href": "interview_questions/brainteasers/wind_effect.html",
    "title": "Wind effect on a plane",
    "section": "",
    "text": "One plane does a round trip from A to B in a straight line. Compare the time of the round trip if there is no wind and if there is a constant wind.\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nYou can either use intuition about the effect of the wind during the trip A-B and during the trip B-A or you can use basic physics by introducing the speed of the plane and the speed of the wind.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nLet‚Äôs assume a constant wind in the direction A towards B.\nFor the intuition part, the wind will make the plane go faster on the trip A-B but slower on the trip B-A. Thus, the trip A-B will take less time than the trip B-A and the advantage of being in the same direction as the wind will weight less than the disadvantage of facing the wind. Therefore, the round trip will take more time if there is wind.\nMathematically, let \\(s\\) be the speed of the plane without wind, \\(w\\) be the speed of the wind and \\(d\\) the distance AB. Since time is additive, if no wind we have \\(t_{\\text{no wind}}=t_{AB} + t_{BA} = \\frac{d}{s} + \\frac{d}{s} = \\frac{2d}{s}.\\) Now, in there is wind in the direction A to B, then the speed on the trip A-B will be \\(s+w\\) and the speed on the trip B-A will be \\(s-w\\). Thus, we have \\(t_{\\text{wind}}=t_{AB} + t_{BA} = \\frac{d}{s+w} + \\frac{d}{s-w} = \\frac{2ds}{s^2-w^2} = \\frac{2d}{s-w^2/s}.\\) Since \\(w^2/s&gt;0\\), we have \\(\\boxed{t_{\\text{wind}} &gt; t_{\\text{no wind}}.}\\)"
  },
  {
    "objectID": "interview_questions/brainteasers/invisible_hats.html",
    "href": "interview_questions/brainteasers/invisible_hats.html",
    "title": "Prisoners random hat",
    "section": "",
    "text": "We have 100 prisoners facing a wall forming a line each wearing a hat either blue or red. The prisoners have to call the color of their hat knowing that they can see those in front of them but not theirs and they can hear the calls of everyone. If a inmate calls the wrong color then he dies. What is the best strategy to save as many prisoners as possible ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nYou cannot be sure that everyone is safe.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nThe best strategy is the following:\n\nThe first one counts the number of red hats in front of him. If this number is even, he calls red. If not, he calls blue. Assuming purely randomness, the probability that this inmate dies is 1/2.\nThe second one knows if the number of red hats (including himself) is even or odd. Then, we counts the number of red hats in front of him. If the parity is the same, he has a blue hat. Otherwise, he has a red hat. This inmate will surely live.\nThe third one will be exactly has the second one. Indeed, based on the call of the second one (if second calls red then one red hat less), he will know the parity of the number of red hats (including himself), he just need to count the ones in front of him to live.\n\nFinally, this strategy surely saves 99 prisoners and lets the final with a probability of 1/2 to live."
  },
  {
    "objectID": "interview_questions/brainteasers/circular_track_ratio.html",
    "href": "interview_questions/brainteasers/circular_track_ratio.html",
    "title": "Computing ratio speed on a circular track",
    "section": "",
    "text": "Assume that two runners are on a circular track. If they run in opposite direction, they meet in 10 minutes. If they run in the same direction, they meet in 60 minutes. What is the ratio of their speed ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nSince you know the times, computing the speed should be easy if you know the distance in each case.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nLet‚Äôs denote by \\(v_1\\) the speed of the fastest and \\(v_2\\) the speed of the other runner. Also, let \\(t_1= 10 \\; \\text{minutes}\\), \\(t_2= 60 \\; \\text{minutes}\\) and \\(d\\) the distance of the track.\n\nIf they run in opposite direction, then when they meet, the distance remaining for one runner is exactly the distance ran by the other. Thus, \\(v_1*t_1 + v_2*t_1 = d\\).\nIf they run in the same direction, then when they meet, the fastest one will have made one more lap that the other. Thus \\(v_1*t_2 = d +  v_2*t_2 .\\)\n\nTherefore, we have \\[\\begin{cases}\n(v_1 + v_2) t_1= d\\\\\n(v_1 - v_2) t_2= d\n\\end{cases}.\\]\nWe then get \\(v_1(t_2-t_1)=v_2(t_1+t_2)\\), accordingly, \\(\\boxed{\\frac{v_1}{v_2}=\\frac{t_1+t_2}{t_2-t_1}=\\frac{7}{5}.}\\)"
  },
  {
    "objectID": "interview_questions/brainteasers/cross_bridge.html",
    "href": "interview_questions/brainteasers/cross_bridge.html",
    "title": "Crossing a bridge in minimum time",
    "section": "",
    "text": "Assume we are during the night and four people need to cross a bridge allowing at most two people to cross at the same time and there is only one flashlight. Knowing that there is one person crossing in 1 minute, one person in 2 minutes, one person in 5 minutes and one person in 10 minutes. What is your strategy so that everyone crosses the bridge in a minimum of time ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nThe fastest need to cross together and the slowest need to cross together.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\n\n1 and 2 cross the bridge (+2 min)\n1 returns (+1 min)\n5 and 10 cross the bridge (+10 min)\n2 returns (+2 min)\n1 and 2 cross the bridge (+2 min)\n\nTotal amount of time 17 minutes."
  },
  {
    "objectID": "interview_questions/brainteasers/squares_grid.html",
    "href": "interview_questions/brainteasers/squares_grid.html",
    "title": "Counting squares in a grid",
    "section": "",
    "text": "Assume we have a \\(n\\times n\\) grid, how many squares are there ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nSeparate all the squares by their size.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\n\n\n\n\n\n\n\n\nSize\nNumber\nExplanation\n\n\n\n\n\\(n\\times n\\)\n1\nThe all grid\n\n\n\\(n-1\\times n-1\\)\n4\nLeaving one row and one column 2*2 possibilites\n\n\n\\(n-2\\times n-2\\)\n9\nLeaving two rows and two columns 3*3 possibilities\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\\(1\\times1\\)\n\\(n^2\\)\nAll little squares\n\n\n\nFinally, \\(N = \\sum_{k=1}^n k^2 = \\frac{N(N+1)(2N+1)}{6}\\)"
  },
  {
    "objectID": "interview_questions/brainteasers/wine_water.html",
    "href": "interview_questions/brainteasers/wine_water.html",
    "title": "Wine in water or water in wine ?",
    "section": "",
    "text": "We have two jars with the same capacity. We take a small volume from the water and pour it into the wine. Then, we take the same volume from the jar with the wine and pour it back into the one with the water. Is there more wine in the water or more water in the wine ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nYour chemistry class might save you.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nThe intuition is that if there was more wine then the concentration of the wine would be higher in the small volume so it would be more wine in the water. Since the two jars have the same capacity, the answer is none.\nLet‚Äôs prove that mathematically. We will denote by \\(E\\) the volume of the water (Eau in French) and by \\(V\\) the volume of the wine (Vin in French).\n\nFirst, we do nothing and we compute the proportion of wine in both jars: \\(p_{E_V} = \\frac{0}{E}=0\\), and \\(p_{V_V} = \\frac{V}{V}= 1.\\)\nWe take a volume \\(v\\) from the water and pour it into the wine: \\(p_{E_V} = \\frac{0}{E-v}=0\\), and \\(p_{V_V} = \\frac{V}{V+v}.\\)\nWe take the same volume \\(v\\) from the jar with the wine and pour it back into the water: \\(p_{E_V} = \\frac{\\frac{V}{V+v}*v}{E}=\\frac{Vv}{E(V+v)}\\), and \\(p_{V_V} = \\frac{V}{V+v}.\\)\n\nFinally we compute the proportion of the water in the wine knowing that the sum of both proportions is equal to one: \\(p^*_{E_V} = \\frac{Vv}{E(V+v)}\\) and \\(p^*_{V_E} = 1 - \\frac{V}{V+v} = \\frac{v}{V+v}\\). Therefore \\(\\boxed{\\frac{p^*_{E_V}}{p^*_{V_E}}=\\frac{V}{E}.}\\)\nThe ratio between the proportion of wine in the water by the proportion of water in the wine is equal to the ratio of the capacity of the jar of wine by the capacity of the jar of water."
  },
  {
    "objectID": "interview_questions/brainteasers/furtive_spy.html",
    "href": "interview_questions/brainteasers/furtive_spy.html",
    "title": "Catching a furtive spy",
    "section": "",
    "text": "Assume a 1D-axis and a spy that is currently at A&gt;0, and each period of time, he moves by an amount B&gt;0, where A and B are unknown. What is your strategy to be sure to capture the spy ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nMake sure that, over time, you systematically try every possible starting position and every possible step size ‚Äî so that whatever the spy‚Äôs (A,B) are, your strategy will eventually match them.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nWe will use a mapping \\(\\mathbb{N} \\rightarrow \\mathbb{N} *\\mathbb{N}\\). More precisely, by induction, we prove that \\(\\forall n \\in \\mathbb{N}, \\exists! \\;(p,q)\\in  \\mathbb{N} *\\mathbb{N}, n=2^p(2q+1).\\) Therefore, for each time step \\(n\\), we compute the corresponding \\(p\\) and \\(q\\) and we position ourselves at \\(\\text{Position} = p + n*q\\).\nFor \\(p=A\\) and \\(q=B\\), we have \\(n_0 = 2^A(2B+1)\\) and at that time, our position is \\(\\text{Position} = p + n_0*q = A + n_0*B\\) which is exactly the position of the spy.\n\n\n\nWhat happens if we don‚Äôt assume anymore that the coefficients are positive ?"
  },
  {
    "objectID": "interview_questions/brainteasers/polyglot.html",
    "href": "interview_questions/brainteasers/polyglot.html",
    "title": "Polyglots in society",
    "section": "",
    "text": "There are 70 people in a city. If we take any pair of people (X,Y), there is at least one language spoken by X and not spoken by Y and at least one language spoken by Y and not spoken by X. What is the minimum number of languages spoken in this town ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nHow many languages speak the person with most languages and how many languages speak the person with least languages ?\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nThe condition about the pair (X,Y) indicates that the subsets of languages spoken by each person are not comparable for the inclusion. Indeed, every person has to speak the same number of languages (If there is one person with one language more, we can find one speaking all the same except for one and the condition is not fulfilled anymore).\nNow, if there are \\(n\\) languages spoken and each person speaks \\(k\\) languages, then the number of different subsets that we can make is \\(\\binom{n}{k}\\). In order to maximize this number when \\(n\\) is fixed, we can use the Pascal‚Äôs triangle and prove that the maximum is for \\(k = \\lfloor n/2 \\rfloor\\). Accordingly we search for $ $\nThus, the minimum number of languages is 8 and each person speaks 4 languages."
  },
  {
    "objectID": "interview_questions/brainteasers/unique_lighter_coin.html",
    "href": "interview_questions/brainteasers/unique_lighter_coin.html",
    "title": "Finding a lighter coin with minimum weightings",
    "section": "",
    "text": "Let‚Äôs suppose that we have 100 coins but one is lighter that the others. Find the minimum number of weightings to identify this fake coin.\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nTry to think about how many outcomes can one weighting differentiate.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nEach weighing can lead to three different outcomes:\n\nThe left part is heavier\nThe left part is lighter\nThe left part and the right part have same weight\n\nNow we can find the strategy:\n\nFirst weighting : 33 vs 33 (34 left)\nSecond weighting:\n\nIf one of the groups was lighter: 11 vs 11 (11 left)\nIf same weight: 11 vs 11 (12 left)\n\nThird weighting:\n\nIf the second case led to same weight : 4 vs 4 (4 left)\nIf not: 4 vs 4 (3 left)\n\nFourth weighting:\n\nIf the second case led to same weight: 1 vs 1 (1 left) ‚úÖ\nIf not : 1 vs 1 (2 left)\n\nFifth weighting:\n\nIf the second case led to different weights: ‚úÖ\nIf not : 1 vs 1 (0 left) ‚úÖ\n\n\nSo the minimum number of weighting to be sure to identify the fake coin in n = 5.\n\n\n\n\n\n\n\n\n\nTipüö® Generalisation\n\n\n\n\n\nWe can try to generalise this strategy. Let‚Äôs assume that there are \\(N\\) coins with one fake. Then since each weighting leads to \\(3\\) different outcomes, we can identify the fake coins with \\(n\\) weightings if \\(3^n&gt;N\\). We want the minimum number of weightings to be sure to identify the coin, thus \\[3^{n-1}&lt;N \\le 3^n.\\] Accordingly, we have \\[n = \\lfloor \\frac {ln(N)}{ln(3)} \\rfloor + 1 .\\] With \\(N=100\\), we find indeed \\(n=5\\)."
  },
  {
    "objectID": "projects/tipe_somnolence/index.html",
    "href": "projects/tipe_somnolence/index.html",
    "title": "Detecting the drowsiness at the wheel",
    "section": "",
    "text": "Made for the national competitive exam to enter the French Engineering Schools, this project is a research on the theme ‚ÄúHealth and Prevention‚Äù and presents two methods to monitor drivers‚Äô drowsiness. The first method uses an ECG sensor to measure the heart rate of the driver and the second implements a CNN to detect if the eyes are open or closed. The data used for this second method is the MRL Eye dataset and can be found here MRL Eye Dataset\n\n\nShow the presentation of the project\n\nDownload PDF file.\n\n\n\nShow the slides for the restitution\n\nDownload PDF file.\n\n\nImport of the dataset\nThe first tasks in order to create the neural network capable to detect the state of the eyes was to download the MRL Eye Dataset and to convert it into a binary format. Indeed, the dataset came with two folders each with each picture of eyes open or closed (in the .jpg format), but, for practical reasons, I wanted to have a unique file with all the pictures. Since, an array with all the images was to heavy, I decided to use the .pickle format in order to store the dataset.\n\n\nShow the dataset creation in the pickle format\n\n##################################################\n#############        IMPORTS         #############\n##################################################\n\nimport numpy as np\nimport cv2\nimport random as rd\nimport pickle          # Conversion python objects into binary\nimport os\nfrom tqdm import tqdm\n\n##################################################\n#############    CREATION DATASET    #############\n##################################################\n\nDATADIR = \"DATASET\"\nCATEGORIES = [\"close\", \"open\"]\nIMG_SIZE = 100\ntraining_data = []\n\ndef create_training_data():\n    for category in CATEGORIES:  \n        path      = os.path.join(DATADIR,category) \n        class_num = CATEGORIES.index(category)  \n        for img in tqdm(os.listdir(path)):\n            img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)\n            new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n            training_data.append([new_array, class_num])  \n            \ncreate_training_data()\nrd.shuffle(training_data)\n\nfeature_list, label_list = zip(*training_data)\nfeature_list, label_list = list(feature_list), list(label_list)\n\nfeature_list = np.array(feature_list).reshape(-1, IMG_SIZE, IMG_SIZE, 1)  # -1 unknown number of images, IMG_SIZE for the shape of the image, 1 for the grayscale\n\n##################################################\n#############      SAVE DATASET      #############\n##################################################\n\npickle_out = open(\"feature_list.pickle\",\"wb\") # wb for writing in binary mode\npickle.dump(feature_list, pickle_out)         # creates a binary copy of feature_list and puts it in pickle_out\npickle_out.close()\n\npickle_out = open(\"label_list.pickle\",\"wb\")\npickle.dump(label_list, pickle_out)\npickle_out.close()\n\n\n\nTraining of the neural networks\nOnce I have my dataset stored in the right format, I can upload it to implement the training of various neural networks. I tried several different structures for the neural network, varying the number of layers in the convolutional layers, the number of neurons per fully-connected layer, the batch_size of the training and finally the number of epochs. What remained constant across all trainings was the kernel of the convolution layers (3x3), the activation function relu, the kernel of the maximum pooling (2x2), the loss function binary_crossentropy, the optimizer sgg and the metric accuracy.\n\n\nShow the training of the neural networks\n\n##################################################\n#############        IMPORTS         #############\n##################################################\n\nimport pickle \nimport cv2\nimport numpy as np\nimport random as rd \nimport os\nimport datetime\n    # Import tensorflow for neural networks\nfrom tensorflow.keras.models    import Sequential\nfrom tensorflow.keras.layers    import Dense, Flatten\nfrom tensorflow.keras.layers    import Conv2D, MaxPooling2D\nfrom tensorflow.keras.callbacks import TensorBoard\n\n\n##################################################\n############  CREATION TEST DATASET  #############\n##################################################\nDATADIR = \"DATASET2\"\nCATEGORIES = [\"close\", \"open\"]\nIMG_SIZE = 100 \ntest_data = []\n\ndef create_test_data():\n    for category in CATEGORIES:  \n        path = os.path.join(DATADIR,category)  \n        class_num = CATEGORIES.index(category)  \n        for img in os.listdir(path):\n            img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  \n            new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  \n            test_data.append([new_array, class_num])  \n            \ncreate_test_data()\nrd.shuffle(test_data)\n\n##################################################\n############  CREATION TEST FUNCTION  ############\n##################################################\n\ndef test_perf():\n    c=0\n    for (image,label) in test_data:\n        image=image.reshape(-1, IMG_SIZE, IMG_SIZE, 1) \n        prediction = model.predict([image])\n        if round(prediction[0][0])==label:\n            c+=1\n    return (c/len(test_data))\n\n##################################################\n##############    LOAD  DATASET    ###############\n##################################################\n\nfeature_in = open(\"feature_list.pickle\",\"rb\")\nfeature_list = pickle.load(feature_in) \nfeature_list = feature_list/255.0                 # Normalize between 0 and 1\n\n\nlabel_in = open(\"label_list.pickle\",\"rb\")\nlabel_list = pickle.load(label_in)\nlabel_list=np.array(label_list)\n\n\n##################################################\n##############    CNN  STRUCTURE    ##############\n##################################################\n\nnetworks=[[256, 256, 128, 128]]\nconvolutions=[[32,32,64]] \nbatch_sizes=[8]\nepochs=[5]\n\n\ndef create_convolution_layers(convolution):\n    for layer in convolution:\n        model.add(Conv2D(layer, (3, 3), activation = 'relu', input_shape = feature_list.shape[1:]))\n        model.add(MaxPooling2D(pool_size = (2, 2)))\n        \ndef create_fully_connected_layers(network):\n    for layer in network:\n        model.add(Dense(layer,activation = \"relu\"))\n\n##################################################\n##############    CNN  TRAININGS    ##############\n##################################################\n\nPERFORMANCE=[]\n\nfor network in networks:\n    for convolution in convolutions:\n        for batch_size_chosen in batch_sizes:\n            for epoch_chosen in epochs: \n                \n                model   = Sequential()\n                create_convolution_layers(convolution)\n                model.add(Flatten())                         # To convert our 3D feature to 1D feature\n                create_fully_connected_layers(network)\n                model.add(Dense(1, activation = \"sigmoid\"))  # Only one neuron: activated=open / inactivated= close\n                model.compile(loss = 'binary_crossentropy', optimizer = 'sgd', metrics = ['accuracy'])\n                run_name = f\"conv{convolution}_net{network}_bs{batch_size_chosen}_ep{epoch_chosen}\"\n                log_dir = \"logs/\" + run_name + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n                tensorboard = TensorBoard(log_dir=log_dir)\n                history = model.fit(feature_list, label_list, batch_size=batch_size_chosen, epochs=epoch_chosen, validation_split=0.1, callbacks=[tensorboard])\n                PERFORMANCE.append(['convolution'] + convolution + ['network'] + network + ['batch_size'] + [batch_size_chosen] + ['epoch'] + [epoch_chosen] + ['test'] + [test_perf()])\n                model.save(\"model.keras\")\n             \nprint(PERFORMANCE) \ninput(\"Press Enter to exit...\")\n\n\n\nTest of the final neural network\nOnce I tried every model, I tested it on 1000 images not in the dataset used for training (either the training dataset or the validation dataset) to keep the models with the best accuracy on unseen data. Finally, I came up with this model: Convolution layers: 32, 32, 64, Fully-Connected Layers:  256, 256, 128, 128, Batch_size: 8, Epoch: 5. Then, I used this neural network to predict if my own eyes were open or closed using my webcam, since that method can be implemented in a car by using a camera focusing on the driver‚Äôs face. However, since the MRL Eye Dataset was only focused on the eyes, predicted on the whole face did not give a good result. If this project were to be deepen, either the dataset should be changed or a function to isolate the eyes from the face should be created.\n\n\n\n\nShow the use of the neural network with a webcam\n\n##################################################\n#############        IMPORTS         #############\n##################################################\n\nimport numpy as np\nimport pickle\nimport cv2\nimport time\nimport tensorflow as tf\nimport winsound\nimport keras\n\n##################################################\n#############       LOAD MODEL       #############\n##################################################\n\nCATEGORIES = [\"Close\", \"Open\"] \nIMG_SIZE = 100 \n\nmodel = keras.models.load_model(\"model.keras\")\n\n##################################################\n#############       WEBCAM USE       #############\n##################################################\n\ncap = cv2.VideoCapture(0)\ncompteur_yeux_fermes = 0\nwhile True:\n    ret, frame = cap.read()\n    start      = time.time()\n    image      = cv2.cvtColor(frame[150:350, 200:400], cv2.COLOR_BGR2GRAY)    # Extract a part of the image\n\n    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE)) \n    image = image.astype(\"float32\") / 255.0 \n    image = image.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n    prediction = model.predict(image)\n    cv2.putText(frame, CATEGORIES[round(prediction[0][0])], (500,30), cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 0), 2)\n    \n    if round(prediction[0][0]) == 0:\n        compteur_yeux_fermes+=1\n    else:\n        compteur_yeux_fermes = 0\n    if compteur_yeux_fermes &gt;= 10:\n        cv2.putText(frame,\"DANGER\", (250,400), cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255), 5)\n        #winsound.Beep(350, 100)\n        #winsound.MessageBeep(type=-1)\n        \n    if cv2.waitKey(1)&0xFF==ord('q'):\n        break\n        \n    fps= 1 / (time.time()-start)\n    cv2.putText(frame, \"FPS: {:4.1f}\".format(fps), (10, 30), cv2.FONT_HERSHEY_PLAIN, 2, (255, 0, 0), 2)\n    cv2.rectangle(frame, (200,150),(400,350),(255,255,255),2)\n    \n    cv2.imshow('Webcam image', frame)\n    \ncap.release()\ncv2.destroyAllWindows()"
  },
  {
    "objectID": "projects/bsm_monte_carlo/index.html",
    "href": "projects/bsm_monte_carlo/index.html",
    "title": "Pricing options with Black-Sholes-Merton and Monte Carlo in C++",
    "section": "",
    "text": "During a elective course at ENSAE Paris as an Introduction to C++, my final project was to implement vanilla options‚Äô pricing using the Black-Scholes-Merton formula and the Monte Carlo simulation approach. The main idea of the project was to use a parent class and then two derived that implemented each method.\n\n\nShow the report of the project\n\nDownload PDF file.\n\n\nDefinition of the original class\nThe main class regroups all the parameters for the Black-Scholes model and coutains a function price() that will be defined later in both methods.\n\n\nShow the main class\n\n\n#include &lt;iostream&gt;\n#include &lt;cmath&gt;\n#include &lt;random&gt;\nusing namespace std;\n\n\nclass Class_Option {\n    protected:\n        double S0;    // initial price\n        double K;     // Strike Price\n        double T;     // Maturity\n        double sigma; // Volatility\n        double r;     // risk-free interest rate\n        bool type;    // call=1 and put=0\n\n    public:\n        Class_Option(double S0, double K, double T, double sigma, double r, bool type); //Constructor definition\n        virtual double price() const = 0;         //any derived class must implement this method to calculate the price of the option\n        virtual ~Class_Option() = default;        //Ensures proper cleanup of derived objects when deleted through a base class pointer\n\n};\n\n//Initializes the attributes of the Option object with user-provided values\nClass_Option::Class_Option(double S0, double K, double T, double sigma, double r, bool type):\n    S0(S0), K(K), T(T), sigma(sigma), r(r), type(type) {}\n\n\n\nDefinition of the Black-Scholes-Merton class\nThis class inherits from the first one and implements the function price() using the Black-Scholes formula for the call price \\(\\boxed{c_t = S_t \\mathcal{N}(d_1)-Ke^{-r(T-t)}\\mathcal{N}(d_2)}\\) with \\[\\begin{cases} d_1 = \\frac{ln(S_t/K)+(r+\\sigma^2/2)(T-t)}{\\sigma \\sqrt{T-t}} \\\\ d_2 = d_1 - \\sigma \\sqrt{T-t}\\end{cases}.\\] To compute the normal cdf we use the error function coded in the namespace std.\n\n\nShow the BSM class\n\nclass Black_Scholes_Merton : public Class_Option {\nprivate:\n    double callPrice() const;\n    double  putPrice() const;\n\npublic:\n    Black_Scholes_Merton(double S0, double K, double T, double sigma, double r, bool type);\n    ~Black_Scholes_Merton();\n    virtual double price() const;\n};\n\n\nBlack_Scholes_Merton::Black_Scholes_Merton(double S0, double K, double T, double sigma, double r, bool type)\n             : Class_Option(S0, K, T, sigma, r, type) {};\n\nBlack_Scholes_Merton::~Black_Scholes_Merton() {}\n\ndouble Black_Scholes_Merton::callPrice() const {\n    double d1 = (std::log(S0 / K) + (r + 0.5 * sigma * sigma) * T) / (sigma * std::sqrt(T));\n    double d2 = d1 - sigma * std::sqrt(T);\n    return S0 * std::erfc(-d1 / std::sqrt(2)) / 2 - K * std::exp(-r * T) * std::erfc(-d2 / std::sqrt(2)) / 2;\n}\n\ndouble Black_Scholes_Merton::putPrice() const {\n    double d1 = (std::log(S0 / K) + (r + 0.5 * sigma * sigma) * T) / (sigma * std::sqrt(T));\n    double d2 = d1 - sigma * std::sqrt(T);\n    return K * std::exp(-r * T) * std::erfc(d2 / std::sqrt(2)) / 2 - S0 * std::erfc(d1 / std::sqrt(2)) / 2;\n}\n\ndouble Black_Scholes_Merton::price() const {\n  if ((*this).type==1){return (*this).callPrice();}\n  else {return (*this).putPrice();}\n}\n\n\n\nDefinition of the Monte_Carlo class\nLikewise, this class inherits from the first one and implements the price() function using Brownian Motion simulation. Indeed, we add an attributes which is the number of simulations that we do, and for each simulation we compute the final value of the Stochastic process of the stock following the dynamics of a Geometric Brownian Motion : \\(\\frac{dS_t}{S_t} = \\mu dt + \\sigma dW_t\\), which leads under the risk-neutral measure to \\[\\boxed{S_T = S_te^{(r-\\sigma^2/2)(T-t) + \\sigma (W_T - W_t)}.}\\] Therefore, we are not generating the whole path of the stock but only its final value thanks to a draw of a normal distribution \\(\\mathcal{N}(0,T-t).\\) Then, we compute the payoff of the option and discount it to take the average as the option‚Äôs price.\n\n\nShow the MC class\n\nclass Monte_Carlo : public Class_Option {\nprivate:\n    int num_simulations;\n\npublic:\n    Monte_Carlo(double S0, double K, double T, double sigma, double r, bool type, int num_simulations);\n    double price() const override;   //Monte Carlo pricing method\n};\n\n\nMonte_Carlo::Monte_Carlo(double S0, double K, double T, double sigma, double r, bool type, int num_simulations)\n    : Class_Option(S0, K, T, sigma, r, type), num_simulations(num_simulations) {}\n\ndouble Monte_Carlo::price() const {\n    std::mt19937 generator(std::random_device{}());\n    std::normal_distribution&lt;double&gt; normal(0.0, 1.0);\n\n    double sum_payoffs = 0.0;\n    for (int i = 0; i &lt; num_simulations; ++i) {\n        double Z = normal(generator);\n        double ST = S0 * std::exp((r - 0.5 * sigma * sigma) * T + sigma * std::sqrt(T) * Z);\n\n        double payoff = 0.0;\n        if (type) {\n            payoff = std::max(ST - K, 0.0);\n        } else {\n            payoff = std::max(K - ST, 0.0);\n        }\n\n        sum_payoffs += payoff;\n    }\n\n    return std::exp(-r * T) * (sum_payoffs / num_simulations);\n}\n\n\n\nComparison of the two methods\nFinally, we can create the main function with the chosen parameters :\\(S_0 = \\$ 100, K = \\$ 102, T = 1 \\; \\text{year}, \\sigma = 0.2 \\; \\text{year}^{-1/2}, r = 0.05 \\; \\text{year}^{-1}.\\) First, we compare the price of the call and the put option with both methods. Also, we verify the put-call parity using the formula from the BSM model and the difference of the simulated prices.\n\n\nShow the computation of the prices\n\nint main() {\n    double S0    = 100.0;   // Initial price\n    double K     = 102.0;   // Strike price\n    double T     = 1.0;     // Time to maturity (1 year)\n    double sigma = 0.2;     // Volatility\n    double r     = 0.05;    // Risk-free rate\n    bool type1   = true;    // True: Call, False: Put\n    bool type2   = false;   // True: Call, False: Put\n    int num_sim  = 1000000 ; //number of simulation\n\n    Black_Scholes_Merton bsm_call(S0, K, T, sigma, r, type1);\n    double BSM_call_price = bsm_call.price();\n\n    Monte_Carlo mc_call(S0, K, T, sigma, r, type1, num_sim);\n    double  MC_call_price = mc_call.price();\n\n    Black_Scholes_Merton bsm_put(S0, K, T, sigma, r, type2);\n    double BSM_put_price  = bsm_put.price();\n\n    Monte_Carlo mc_put(S0, K, T, sigma, r, type2, num_sim);\n    double  MC_put_price  = mc_put.price();\n\n    double expectedParity = S0 - K * std::exp(-r * T);\n\n\n    // Test de tous les prix possibles\n    std::cout &lt;&lt; \"Black-Scholes-Merton Call Price: \" &lt;&lt; BSM_call_price &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Monte Carlo Call Price: \"          &lt;&lt; MC_call_price  &lt;&lt; std::endl &lt;&lt; std::endl;\n\n    std::cout &lt;&lt; \"Black-Scholes Put Price: \"         &lt;&lt; BSM_put_price  &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Monte Carlo Put Price: \"           &lt;&lt; MC_put_price   &lt;&lt; std::endl &lt;&lt; std::endl;\n\n    std::cout &lt;&lt; \"Expected  Put-Call Parity (BSM formula for call - put) \"&lt;&lt; expectedParity               &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Simulated Put-Call Parity (MC  formula for call - put) \"&lt;&lt; MC_call_price - MC_put_price &lt;&lt; std::endl;\n\n    return 0;\n}\n\nHere are the outputs that show that the Monte Carlo approach gives a good approximation of the Black-Scholes-Merton model.\nBlack-Scholes-Merton Call Price: 9.42337\nMonte Carlo Call Price: 9.42634\n\nBlack-Scholes Put Price: 6.44877\nMonte Carlo Put Price: 6.44635\n\nExpected Put-Call Parity (BSM formula for call - put): 2.9746\nSimulated Put-Call Parity (MC formula for call - put): 2.97999\n\nProcess returned 0 (0x0)   execution time : 0.888 s\nPress any key to continue."
  },
  {
    "objectID": "interview_questions/probabilities/local_minima.html",
    "href": "interview_questions/probabilities/local_minima.html",
    "title": "Local minima in a permutation",
    "section": "",
    "text": "What is the expected number of local minima in a random permutation ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nHow can every integer can be a local minimum in the permutation ?\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nLet‚Äôs suppose that we have a permutation with \\(n\\) elements: \\(u \\in \\mathbb{S}_n.\\)\n\nFor \\(k \\in {2, ..., n-1}\\), k is a local minimum if \\(u(k)\\le u(k-1)\\) and \\(u(k)\\le u(k+1).\\) Since the permutation is random each of the three number could be the smallest one with equiprobability. Thus \\(\\forall \\; 2 \\le k \\le n-1, \\mathbb{P}(k \\; \\text{local minimum}) = 1/3.\\)\nFor \\(k=1\\), k is a local minimum if \\(u(k)\\le u(k+1).\\) Thus, \\(\\mathbb{P}(k \\; \\text{local minimum}) = 1/2.\\)\nFor \\(k=n\\), k is a local minimum if \\(u(k)\\le u(k-1).\\) Thus, \\(\\mathbb{P}(k \\; \\text{local minimum}) = 1/2.\\)\n\nFinally, we have \\[\\boxed{\\mathbb{E}[N] = (n-2)* \\frac{1}{3} + 2 * \\frac{1}{2} = \\frac{n+1}{3}.}\\]"
  },
  {
    "objectID": "interview_questions/probabilities/bayes_coin.html",
    "href": "interview_questions/probabilities/bayes_coin.html",
    "title": "Fake coin probability",
    "section": "",
    "text": "We have 3 fair coins and 1 double-headed coin. We take one coin randomly, then we flip it and we get head. What is the probability that we picked the double-headed coin ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nUse Bayes formula \\(\\mathbb{P}(A|B) = \\frac{\\mathbb{P}(A)}{\\mathbb{P}(B)}*\\mathbb{P}(B|A),\\) with the right events.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nLet D the event ‚ÄúWe have the double-headed coin‚Äù and H the event ‚ÄúWe drew a head‚Äù. We want to find \\(\\mathbb{P}(D|H).\\)\nBy the Bayes formula we have \\[\\mathbb{P}(D|H) = \\frac{\\mathbb{P}(D)*\\mathbb{P}(H|D)}{\\mathbb{P}(H)} = \\frac{\\mathbb{P}(D)*\\mathbb{P}(H|D)}{\\mathbb{P}(D)*\\mathbb{P}(H|D) + \\mathbb{P}(\\bar{D})*\\mathbb{P}(H|\\bar{D})}.\\]\nTherefore, we get \\[\\mathbb{P}(D|H) = \\frac{\\frac{1}{4}*1}{\\frac{1}{4}*1 + \\frac{3}{4}*\\frac{1}{2}} = \\frac{\\frac{1}{4}}{\\frac{5}{8}}=\\frac{2}{5}.\\]"
  },
  {
    "objectID": "interview_questions/probabilities/chevalier_mere.html",
    "href": "interview_questions/probabilities/chevalier_mere.html",
    "title": "Knight of Mere",
    "section": "",
    "text": "What is the most probable draw at least one 6 in 4 throws with one dice or draw at least a double 6 in 24 throws with two dice ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nTry to go backwards by supposing one inequality and use the concavity of the logarithm function to prove it.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nLet A ‚ÄúHave at least one 6 in 4 throws with 1 dice‚Äù and B ‚ÄúHave at least a double 6 in 24 throws with 2 dice‚Äù.\nWe have\n\\[\\begin{align*}\n\\mathbb{P}(A)&gt;\\mathbb{P}(B)\n&\\iff 1-\\left(\\frac{5}{6}\\right)^4&gt;1-\\left(\\frac{35}{36}\\right)^{24} \\\\\n&\\iff \\left(\\frac{5}{6}\\right)^4 &lt; \\left(\\frac{35}{36}\\right)^{24} \\\\\n&\\iff 36^{24}5^4 &lt; 35^{24}6^4 \\\\\n&\\iff 6^{48}5^4 &lt; 5^{24}7^{24} \\\\\n&\\iff 6^{44}&lt;5^{20}7^{24} \\\\\n&\\iff 6^{11} &lt; 5^57^6 \\\\\n&\\iff \\left(\\frac{6}{5}\\right)^5 &lt; \\left(\\frac{7}{6}\\right)^6 \\\\\n&\\iff 5\\ln\\left(\\frac{6}{5}\\right) &lt; 6\\ln\\left(\\frac{7}{6}\\right) \\\\\n&\\iff \\frac{5}{6}\\ln\\left(\\frac{6}{5}\\right)-\\ln\\left(\\frac{7}{6}\\right) &lt; 0.\n\\end{align*}\\]\nNow, using the concavity of the ln, we have \\[\\forall (x,y) \\in \\mathbb{R}_+^* \\times \\mathbb{R}_+^*, \\forall \\lambda \\in (0, 1), ln(\\lambda x + (1 - \\lambda) y )\\ge \\lambda ln(x) + (1 - \\lambda) ln(y).\\]\nWith \\(\\lambda = \\frac{5}{6}, x = \\frac{6}{5}, y=1\\), the inequality becomes \\(ln(\\frac{5}{6} * \\frac{6}{5} + \\frac{1}{6}*1)= ln(\\frac{7}{6}) &gt; \\frac{5}{6}ln(\\frac{6}{5}) + \\frac{1}{6}*ln(1).\\)\nAccordingly, thanks to the concavity inequality drawing at least one 6 in 4 throws with one dice is more likely than drawing at least a double 6 in 24 throws with two dice."
  },
  {
    "objectID": "interview_questions/probabilities/modes_poisson.html",
    "href": "interview_questions/probabilities/modes_poisson.html",
    "title": "Modes of a Poisson distribution",
    "section": "",
    "text": "What is the mode of \\(\\mathcal{P}(\\lambda)\\)?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nLet \\(X \\sim \\mathcal{P}(\\lambda)\\) and compute the ratio \\(\\frac{\\mathbb{P}(X = k+1)}{\\mathbb{P}(X=k)}.\\)\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nWe have \\(\\frac{\\mathbb{P}(X = k+1)}{\\mathbb{P}(X=k)} = \\frac{\\lambda^{k+1}e^{-\\lambda}}{(k+1)!} * \\frac{k!}{\\lambda^{k}e^{-\\lambda}} = \\frac{\\lambda}{k+1}\\)\nTherefore, we get\n\nIf \\(\\lambda &lt; k+1\\), then \\(\\mathbb{P}(X = k+1)&lt;\\mathbb{P}(X = k)\\)\nIf \\(\\lambda &gt; k+1\\), then \\(\\mathbb{P}(X = k+1)&gt;\\mathbb{P}(X = k)\\)\nIf \\(\\lambda = k+1\\), then \\(\\mathbb{P}(X = k+1)=\\mathbb{P}(X = k)\\)\n\nAccordingly, if \\(\\lambda \\in \\mathbb{N},\\) then the mode is \\(\\lambda\\) and \\(\\lambda -1\\) (they have the same probability) and if \\(\\lambda \\notin \\mathbb{N},\\) then the mode is \\(\\lfloor \\lambda \\rfloor.\\)"
  },
  {
    "objectID": "interview_questions/probabilities/maximum_equal_bernoulli.html",
    "href": "interview_questions/probabilities/maximum_equal_bernoulli.html",
    "title": "Maximum of probability equality two Bernoulli",
    "section": "",
    "text": "If \\(X \\sim \\mathcal{B}(p)\\) and \\(Y \\sim \\mathcal{B}(q)\\), what is the maximum value of \\(\\mathbb{P}(X=Y)\\) ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nIf the random variables would be independant, it would trivial. Also, fix \\(a = \\mathbb{P}(X = 1 \\cap Y = 1)\\), to compute all the probabilities.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nIf \\(X \\perp Y\\), then \\(\\mathbb{P}(X=Y) =  \\mathbb{P}(X=1)\\mathbb{P}(Y=1)+\\mathbb{P}(X=0)\\mathbb{P}(Y=0) = qp + (1-p)(1-q) = 1 - p - q.\\)\nNow, let‚Äôs suppose that \\(X \\not\\perp Y\\) and let \\(a = \\mathbb{P}(X = 1 \\cap Y = 1)\\).\nWe have\n\n\\(\\mathbb{P}(X = 1 \\cap Y = 1) = a\\)\n\\(\\mathbb{P}(X = 1 \\cap Y = 0) = \\mathbb{P}(X = 1) - \\mathbb{P}(X = 1 \\cap Y = 1) = p - a\\)\n\\(\\mathbb{P}(X = 0 \\cap Y = 1) = \\mathbb{P}(Y = 1) - \\mathbb{P}(X = 1 \\cap Y = 1) = q - a\\)\n\\(\\mathbb{P}(X = 0 \\cap Y = 0) = 1 - a - (p-a) - (q-a) = 1 - p - q + a\\)\n\nThus, \\(\\boxed{\\mathbb{P}(X=Y) = 1 - p - q + 2a}\\), this function is increasing in \\(a\\) so the maximum value is obtained for the maximum value of \\(a\\). Since all the probabilities are positive, we have \\[\\begin{cases}a \\le p \\\\ a \\le q \\\\ a \\ge 0 \\\\ a \\ge (p+q)-1 \\end{cases}.\\] Thus we have\n\\[\\boxed{\\begin{cases} a \\le min(p, q) \\\\ a \\ge max(0, p+q-1) \\end{cases}.}\\]\nAccordingly, \\[\\boxed{max \\mathbb{P}(X=Y) = 1 - (p+q)+2\\, min(p,q) = 1 - |p-q|.}\\]"
  },
  {
    "objectID": "interview_questions/probabilities/key_last_room.html",
    "href": "interview_questions/probabilities/key_last_room.html",
    "title": "Lost key in the last classroom",
    "section": "",
    "text": "The keeper of a high school lost the key of the school in one of the 123 rooms with a probability \\(p\\). He already searched the key in 122 rooms and it wasn‚Äôt there. What is the probability that the lost key is in the last room ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nIf we were sure that the key was in the high school then it will surely be in the last classroom. But, what happens if we are not sure that the key is even in the school.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nLet‚Äôs denote three different events:\n\nA: ‚ÄúThe key is in the school‚Äù \\(\\mathbb{P}(A) = p\\)\nB: ‚ÄúThe key is in the last classroom‚Äù \\(\\mathbb{P}(B) = \\mathbb{P}(A)*\\mathbb{P}(B|A) + \\mathbb{P}(\\bar{A})*\\mathbb{P}(B|\\bar{A}) = p*\\frac{1}{123}+(1-p)*0 = \\frac{p}{123}\\)\nC: ‚ÄúThe key is not found in the first 122 rooms‚Äù: \\(\\mathbb{P}(C) = \\mathbb{P}(A)*\\mathbb{P}(C|A) + \\mathbb{P}(\\bar{A})*\\mathbb{P}(C|\\bar{A}) = p*\\frac{1}{123}+(1-p)*1 = 1-p+\\frac{p}{123}\\)\n\nThe problem asks for \\(\\mathbb{P}(B|C).\\)\nBy Bayes‚Äôs theorem, we have \\(\\mathbb{P}(B|C) = \\mathbb{P}(C|B)\\frac{\\mathbb{P}(B)}{\\mathbb{P}(C)}.\\)\nAccordingly, we get \\(\\boxed{\\mathbb{P}(B|C) = 1*\\frac{\\frac{p}{123}}{1-p+\\frac{p}{123}}=\\frac{p}{123-122p}.}\\)\nIf we are sure that the keeper lost his key in the school (ie \\(p=1\\)), then we find that the key is surely in the last classroom \\(\\mathbb{P}(B|C) = 1.\\)"
  },
  {
    "objectID": "interview_questions/probabilities/parity_dice_sum.html",
    "href": "interview_questions/probabilities/parity_dice_sum.html",
    "title": "Parity of the sum of dices",
    "section": "",
    "text": "Assume that we have \\(n\\) different dices each with \\(20\\) faces. What is the probability that the sum is even ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nWhat is the probability that one dice shows an even number ? Then, for \\(n\\) independant draws ?\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nLet the final sum be \\(S_n = \\sum_{i=1}^n X_i\\) with \\(\\mathbb{P}(X_i=1) = \\mathbb{P}(D_i \\in 2\\mathbb{N}) = 1/2\\) and \\(\\mathbb{P}(X_i=0) = \\mathbb{P}(D_i \\in 2\\mathbb{N}+1) = 1/2.\\) Therefore, \\(\\boxed{\\mathbb{P}(S_n \\in 2\\mathbb{N}) = \\sum_{i \\; \\text{even}} \\binom{n}{i}*2^{-n}= 1/2.}\\)"
  },
  {
    "objectID": "interview_questions/probabilities/selected_colors.html",
    "href": "interview_questions/probabilities/selected_colors.html",
    "title": "Polya‚Äôs urn",
    "section": "",
    "text": "An urn initially contains b white balls and r red balls. The following experiment is carried out: * A ball is drawn from the urn at random, and its color is noted. * The ball is returned to the urn, and d balls of the same color are added. The experiment is repeated as many times as desired. Let \\(n \\ge 1\\). Determine the probability that the ball drawn will be white on the nth draw.\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nCompute the probability for the first draw, the second, the third. Then make a guess for the general formula and prove it by induction.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nWe denote by \\(W_n\\) the event the a white ball is drew from the nth draw. Let \\(P_n : \\mathbb{P}(W_n)=\\frac{b}{b+r}\\)\n\nInitialisation : for \\(n=1\\), there are \\(b\\) white balls among the \\(b+r\\) total balls so the formula is correct at the step 1\nHeredity: Let \\(n \\ge 1\\). Assume that the result is true for \\(n \\ge 1\\).We cannot use \\(\\mathbb{P}(W_{n+1}  | W_n)\\) in our calculations, because we do not know exactly how many balls are in the urn when \\(W_n\\) is true. Except for \\(W_1\\). After one draw, if \\(W_1\\) is true, we have $ b + d $ white balls and $ r$ red balls. To reach \\(n + 1\\) draws, there remain \\(n\\) draws to perform. This is exactly equivalent to performing \\(n\\) draws starting with \\(b_0 = b + d\\) white balls and \\(r_0 = r\\) red balls. Thus, by the induction hypothesis, we deduce: \\(\\mathbb{P}(W_{n+1} | W_1) = \\frac{b_0}{b_0 + r_0} = \\frac{b + d}{b + d + r}\\). Likewise, in the other case, we have \\(\\mathbb{P}(W_{n+1} | \\bar{W_1}) = \\frac{b}{b + d + r}.\\) Accordingly, \\(\\mathbb{P}(W_{n+1}) = \\mathbb{P}(W_{n+1} \\mid W_1)\\mathbb{P}(W_1) + \\mathbb{P}(W_{n+1} \\mid \\bar{W_1})\\mathbb{P}(\\bar{W_1})] = \\frac{b + d}{b + d + r} \\times \\frac{b}{b + r} + \\frac{b}{b + d + r} \\times \\frac{r}{b + r}.\\) Finally, \\(\\mathbb{P}(W_{n+1})=\\frac{b}{b+r}\\)\n\nBy the principle of the induction proof, \\(\\boxed{\\forall n \\in \\mathbb{N}^*, \\mathbb{P}(W_n)=\\frac{b}{b+r}}\\)"
  },
  {
    "objectID": "interview_questions/probabilities/drunk_postman.html",
    "href": "interview_questions/probabilities/drunk_postman.html",
    "title": "Drunk postman",
    "section": "",
    "text": "This problem studies the case of a drunk postman who has to deliver \\(n\\) letter and \\(n\\) boxes but assigns each letter randomly.\n\nQuestion 1: Right permutation\nWhat is the probability that each letter goes to the right box ?\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nSince \\(Card(\\mathcal{S}_n) = n!\\), we have \\(\\boxed{\\mathbb{P}( \\sigma=id)=\\frac{1}{n!}}\\)\n\n\n\n\n\nQuestion 2: At least one letter right\nWhat is the probability that at least one letter goes to the right box ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nDefine the event \\(A_i\\) : ‚ÄúThe letter i goes to the box i‚Äù and use the Poincare formula for the probability of the union.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nDefine the event \\(A_i\\) : ‚ÄúThe letter i goes to the box i, since a good permutation let‚Äôs i unchanged and randomly permutes the others, we get \\(\\mathbb{P}(A_i) = \\frac{(n-1)!}{n!} = \\frac{1}{n}\\)\nBy the Poincare formula, we find\n\\(\\mathbb{P}(\\bigcup_{i=1}^n A_i) = \\sum_{k=1}^n (-1)^{k-1} \\sum_{1\\le i_1 &lt; ... &lt; i_k  \\le n} \\mathbb{P}(A_{i_1} \\cap ... \\cap A_{i_k})=\\sum_{k=1}^n (-1)^{k-1}\\binom{n}{k}\\frac{(n-k)!}{n!} = \\sum_{k=1}^n \\frac{(-1)^{k-1}}{k!}.\\)\nAccordingly, we have \\[\\boxed{\\mathbb{P}(\\text{at least one good letter})=1-\\sum_{k=0}^n \\frac{(-1)^{k}}{k!} \\rightarrow_{n\\to\\infty} 1-e^{-1}\\approx0.632.}\\]\n\n\n\n\n\nQuestion 3: No right letter\nWhat is the probability that no letters are delivered to the right box ?\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nWe have \\[\\boxed{\\mathbb{P}(\\text{no good letter})= 1-\\mathbb{P}(\\text{at least one good letter})=e^{-1} \\approx 0.368.}\\]\nWe can find the number of ways that no letter are right delivered: probability one permutation leeds to no good letter * number of permutations : \\(d_n = n!e^{-1}.\\)"
  },
  {
    "objectID": "interview_questions/probabilities/min_max_uniform.html",
    "href": "interview_questions/probabilities/min_max_uniform.html",
    "title": "Operation on Uniform distributions",
    "section": "",
    "text": "Let‚Äôs suppose that we have two fair dices ( \\(U_1 \\sim \\mathcal{U}(\\{1,...,6\\}), U_2 \\sim \\mathcal{U}(\\{1,...,6\\}), U_1 \\perp U_2\\)). Let \\(X = min(U_1, U_2) \\; \\text{and} \\; Y = max(U_1, U_2).\\)\n\n\\(\\mathbb{E}[X]\\) ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nFind \\(\\mathbb{P}(X=k)\\) and compute the expectation\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nLet \\(1 \\le k\\le6\\), we have \\((X=k) = (U_1 = k \\; \\cap \\; U_2 = k) \\cup (U_1 = k \\; \\cap \\; U_2 &gt; k) \\cup (U_1 &gt; k \\; \\cap \\; U_2 = k).\\) Out of the 36 possible outcomes, the first event happens \\(6\\) times, the second \\(6-k\\) times and the third also \\(6-k\\) times. Thus \\(\\boxed{\\mathbb{P}(X=k) = \\frac{1}{36}+ \\frac{6-i}{36}+ \\frac{6-i}{36}=\\frac{13-2i}{36}}.\\)\nFinally, we compute \\[\\boxed{\\mathbb{E}[X] = \\sum k\\mathbb{P}(X=k) = \\frac{11}{36}+2*\\frac{9}{36}+3*\\frac{7}{36}+4*\\frac{5}{36}+5*\\frac{3}{36}+6*\\frac{1}{36} = \\frac{91}{36}\\approx 2.52}\\]\n\n\n\n\n\n\\(\\mathbb{E}[Y]\\) ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nWe don‚Äôt need to do the same as in the first question.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nWe have \\(X + Y =U_1 + U_2\\), thus by linearity of the expectation, \\[\\boxed{\\mathbb{E}[Y] = \\mathbb{E}[U_1] + \\mathbb{E}[U_2]-\\mathbb{E}[X] = \\frac{7}{2} + \\frac{7}{2} - \\frac{91}{36} = \\frac{161}{36}}\\]\n\n\n\n\n\n\\(cov(X,Y)\\) ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nUse a similar trick as in the second question.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nWe have \\(cov(X,Y) = \\mathbb{E}[XY] -\\mathbb{E}[X]\\mathbb{E}[Y] = \\mathbb{E}[U_1U_2] -\\mathbb{E}[X]\\mathbb{E}[Y] = \\mathbb{E}[U_1]\\mathbb{E}[U_2] -\\mathbb{E}[X]\\mathbb{E}[Y].\\)\nThus, \\[\\boxed{cov(X,Y) = \\frac{7}{2} \\frac{7}{2} - \\frac{91}{36} \\frac{161}{36} = \\frac{1225}{1296}.}\\]"
  },
  {
    "objectID": "interview_questions/probabilities/stick_triangle.html",
    "href": "interview_questions/probabilities/stick_triangle.html",
    "title": "Triangle from a broken stick",
    "section": "",
    "text": "We have a stick of length 1 and it breaks. What is the probability that the broken pieces form a triangle ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nParametrize the problem with the length of the broken pieces and compute the partial area corresponding to a triangle.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nLet \\(x\\) be the length of the first piece and \\(y\\) the second piece (thus the final length is \\(1-x-y\\))\nSince all length are positive, the possible region is \\[\\begin{cases} x \\ge 0 \\\\ y \\ge 0 \\\\ y \\le 1 -x \\end{cases}.\\]\nThe broken pieces form a triangle if and only if all length are lower than the sum of the other sides. Thus we have \\[\\begin{cases} x \\le y + 1 -x -y \\\\  y \\le y + 1 -x -y \\\\ 1 -x - y \\le x + y  \\end{cases} \\sim \\begin{cases} x \\le 1/2 \\\\  y \\le 1/2 \\\\ y \\ge 1/2 -x   \\end{cases}.\\]\nWe can plot the regions and we find \\(\\boxed{\\mathbb{P}(\\text{Triangle}) = \\frac{A_{tri}}{A_{tot}}=\\frac{1/8}{1/2}=1/4.}\\)"
  },
  {
    "objectID": "interview_questions/probabilities/random_boxes.html",
    "href": "interview_questions/probabilities/random_boxes.html",
    "title": "Random boxes",
    "section": "",
    "text": "We have 100 people each assigned a box. Create a strategy where each person can open 50 boxes that has at least 30 % chance of success.\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nA permutation can always be decomposed into a product of disjoint cycles.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nIf the strategy was full random, each person would have a chance of 1/2 to find his box. Therefore, the probability of the success is \\((1/2)^{100} &lt;&lt;1.\\)\nLet‚Äôs assume that each person has a number between 1 and 100, the boxes have a number written on them and a number inside them. The goal is that each person has the box with their number inside. Let‚Äôs start by giving them the box with their number outside. Then, if the number inside is not their, they would pick the box with this given number on the outside. Mathematically, we have a permutation \\(\\sigma()\\) and for each person that does not have its number we will compute \\(\\sigma^2()\\).\nIf after 50 iterations, the person i has not his box, that means that the permutation countains one cycle of length 51 or more (it is at least one but since the length is higher that the have, it is only one). Accordingly, \\(\\mathbb{P}(\\text{success}) = 1 - \\sum_{k=51}^{100} \\mathbb{P}(\\text{cycle length k})\\).\nNow, in order to compute a cycle of length k, we need to choose the k elements that are in the cycle, then a permutation of the remaining numbers. However, many permutations represent the same cycle (1 for each position of one element). Indeed, the permutations (1, 2, 3), (2, 3, 1), (3, 1, 2) represent the same cycle so we need to divide the number of permutations of the k elements by k. Thus \\(\\mathbb{P}(\\text{cycle length k}) =  \\binom{n}{k}*\\frac{k!}{k}*(n-k)! / n!= 1/k\\),\nFinally, we get \\(\\boxed{\\mathbb{P}(\\text{success}) = 1 - \\sum_{k=51}^{100} 1/k \\approx 1 - (ln(100)-ln(50)) = 1 -ln(2) \\approx 0.31.}\\)"
  },
  {
    "objectID": "interview_questions/probabilities/expected_flips_different.html",
    "href": "interview_questions/probabilities/expected_flips_different.html",
    "title": "Expected coin flips until different result",
    "section": "",
    "text": "We have a fair coin and we flip it until we find a different outcome from the previous one. What is the expected number of flips that we need to do ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nWhat is the sequence of the outcomes so that we need n flips ? Also, compute the expectation using power series expansion.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nLet‚Äôs denote by \\(N\\) the total number of flips. The only two outcomes are \\(H_1, H_2, ..., H_{n-1}, T_n\\) (if the first is a head) and \\(T_1, T_2, ..., T_{n-1}, H_n\\) (if the first is a tail). Thus, \\(\\mathbb{P}(N=n) = \\frac{1}{2^n} + \\frac{1}{2^n} = \\frac{1}{2^{n-1}}.\\)\nAccordingly, \\(\\mathbb{E}[N] = \\sum_{n=2}^{+\\infty} \\frac{n}{2^{n-1}}.\\)\nBy observing that \\(\\frac{d\\frac{1}{1-x}-x}{dx} = \\frac{d \\sum_{n=2}^{+\\infty}x^n}{dx}= \\sum_{n=2}^{+\\infty}nx^{n-1}\\) and \\(\\frac{d\\frac{1}{1-x}-x}{dx} = \\frac{1}{(1-x)^2}-1\\). By specifying in \\(\\boxed{x=1/2}\\), we get \\(\\boxed{\\mathbb{E}[N] = \\frac{1}{(1/2)^2}-1=3.}\\)"
  },
  {
    "objectID": "interview_questions/probabilities/uncertain_meeting.html",
    "href": "interview_questions/probabilities/uncertain_meeting.html",
    "title": "Uncertain Meeting",
    "section": "",
    "text": "Two people agreed to meet between 1 PM and 2 PM. Each one comes randomly in this interval and stays for 30 minutes. What is the probability that they actually meet ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nParametrize the problem using two uniform distributions and compute the partial area corresponding to the meeting.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nLet \\(X\\sim \\mathbb{U}([0,1]), Y\\sim \\mathbb{U}([0,1]), X \\perp Y\\) be the time or arrival of the two people. We want to compute \\(\\mathbb{P}(|X-Y| \\le 1/2)\\)\nSince the two uniforms are positive, the possible region is \\[\\begin{cases} x \\ge 0 \\\\ y \\ge 0\\end{cases}.\\]\nThe meeting happens if\n\\[\\begin{cases} x - y\\le 1/2\\\\  y -x \\le 1/2  \\end{cases} \\sim \\begin{cases} y \\ge x-1/2 \\\\  y \\le x +1/2  \\end{cases}.\\]\nWe can plot the regions and we find \\(\\boxed{\\mathbb{P}(\\text{Meet}) = \\frac{1-2*\\frac{1/2*1/2}{2}}{1}= 3/4.}\\)"
  },
  {
    "objectID": "interview_questions/probabilities/Bullet_cylinder.html",
    "href": "interview_questions/probabilities/Bullet_cylinder.html",
    "title": "Russian bullet cylinder",
    "section": "",
    "text": "A criminal but two bullets in a row in a six bullet cylinder. He shuffles it and shot once, nothing happens. Is it better for you to reshuffle or not ?\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nImagine that the bullets are in slot 1 and 2 and that the cylinder goes increasingly, the cycle is therefore (1, 2, 3, 4, 5, 6).\n\nIf you reshuffle, every slot is equally plausible and \\(\\mathbb{P}(\\text{death | shuffle)} = 1/3\\).\nIf you don‚Äôt reshuffle, since the first shot was blank, you know that you are not in slots 1 nor 2. Among the 4 slots left, only the slot 6 would lead to a bullet at the next shot. Therefore, \\(\\mathbb{P}(\\text{death | no shuffle)} = 1/4.\\)\n\nThus, it is better to not reshuffle."
  },
  {
    "objectID": "interview_questions/probabilities/uniform_sum.html",
    "href": "interview_questions/probabilities/uniform_sum.html",
    "title": "Uniform Sum",
    "section": "",
    "text": "You have a fair dice and a loaded dice such that the sum leads to an uniform distribution. How your dice is fake ?\n\n\n\n\n\n\nTipüí° Hint\n\n\n\n\n\nUse the edge cases.\n\n\n\n\n\n\n\n\n\nTipüí° Solution\n\n\n\n\n\nWe have \\(X \\sim \\mathcal{U}\\{1,...,6\\}\\) and \\(Y\\) such that \\(X +Y \\sim \\mathcal{U}\\{1,...,12\\}.\\)\nThat means that \\(\\forall 1 \\le k \\le 12, \\mathbb{P}(X+Y=k) = 1/12.\\) For \\(k=1\\), since the lowest value for X is 1, we know that Y has to be 0. Moreover, \\(\\mathbb{P}(X+Y=1) = \\mathbb{P}(X=1)*\\mathbb{P}(Y=0)\\) so \\(\\boxed{\\mathbb{P}(Y=0)=\\frac{1/12}{1/6}=1/2.}\\) Likewise, since the highest value for X is 6, if X+Y is 12 then Y has to be 6. Thus, \\(\\mathbb{P}(X+Y=12) = \\mathbb{P}(X=6)*\\mathbb{P}(Y=6)\\) so \\(\\boxed{\\mathbb{P}(Y=6)=\\frac{1/12}{1/6}=1/2.}\\)\nFinally, the faces of the fake dice are (0, 0, 0, 6, 6, 6)."
  }
]